{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d83ed8-43d0-458f-9a5c-d0833e60d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from run_model import evaluate_model, train_one_epoch\n",
    "from run_model import save_torchscript_model, load_torchscript_model\n",
    "from datasets import tfds_data_loader, data_loader\n",
    "import resmlp\n",
    "\n",
    "import torch.optim as optim\n",
    "from timm.models import create_model\n",
    "from timm.data import Mixup\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "from timm.utils import NativeScaler, get_state_dict, ModelEma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed33903-ca0b-46b1-a3f9-05a63e6f9c23",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "385bb9a0-43c6-434f-8ce5-b97da9b1fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 224\n",
    "DICT_PATH = 'E:/ResMLP_QAT/pytorch/fp32_weights/ResMLP_S24_ReLU_fp32_80.602.pth' \n",
    "\n",
    "DATA_NAME = 'imagenet2012'\n",
    "DATA_DIR = 'E:\\datasets'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LR = 1e-4\n",
    "\n",
    "WORKERS = 0 #8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6336c16-741a-417d-b9c0-40fcb9309ff3",
   "metadata": {},
   "source": [
    "## Speed Up Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea98493-73af-4a5a-9ef0-2e6cf73277f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Parallel Training (DPT) settings\n",
    "\n",
    "# device = CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set seed\n",
    "seed = 336 # args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# check for best cudnn ops before training starts.\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba8fdb-1cd7-4953-b936-34dbdb52da26",
   "metadata": {},
   "source": [
    "## Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45f5c7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=False){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "Training Model with QAT...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedResMLP(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (module): resmlp_models(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(\n",
       "        3, 384, kernel_size=(16, 16), stride=(16, 16)\n",
       "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): layers_scale_mlp_blocks(\n",
       "        (norm1): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn): Linear(\n",
       "          in_features=196, out_features=196, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): Linear(\n",
       "          in_features=384, out_features=384, bias=True\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): LinearReLU(\n",
       "            in_features=384, out_features=1536, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (act): Identity()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(\n",
       "            in_features=1536, out_features=384, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (gamma_2): Linear(\n",
       "          in_features=384, out_features=384, bias=False\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Linear(\n",
       "      in_features=384, out_features=384, bias=True\n",
       "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (head): Linear(\n",
       "      in_features=384, out_features=1000, bias=True\n",
       "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QuantizedResMLP(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(QuantizedResMLP, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.module(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# build train/val dataset\n",
    "# create sampler (if dataset from tfds, can't apply sampler) (distributed ver. to be done)\n",
    "# build up dataloader\n",
    "data_loader_train, data_loader_val, NUM_CLASSES = tfds_data_loader(\n",
    "    name=DATA_NAME,\n",
    "    root=DATA_DIR,\n",
    "    input_size=INPUT_SIZE, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=WORKERS,\n",
    ")\n",
    "\n",
    "# additional data augmentation (mixup)\n",
    "mixup_fn = Mixup(\n",
    "    mixup_alpha=0.8, cutmix_alpha=1.0, cutmix_minmax=None,\n",
    "    prob=1.0, switch_prob=0.5, mode='batch',\n",
    "    label_smoothing=0.1, num_classes=NUM_CLASSES)  \n",
    "\n",
    "# create model\n",
    "float_model = create_model('resmlp_24', num_classes=NUM_CLASSES).to(device)\n",
    "float_model = load_model(float_model, DICT_PATH, device)\n",
    "\n",
    "# fuse\n",
    "for basic_block_name, basic_block in float_model.blocks.named_children():\n",
    "  for sub_block_name, sub_block in basic_block.named_children():\n",
    "    if sub_block_name == \"mlp\":\n",
    "      torch.quantization.fuse_modules(\n",
    "        sub_block, [['fc1', 'act']],\n",
    "        inplace=True)\n",
    "\n",
    "# apply quant/dequant stabs\n",
    "#float_model1 = torch.quantization.add_quant_dequant(float_model)\n",
    "float_model = QuantizedResMLP(module=float_model)\n",
    "\n",
    "# quantization configurations\n",
    "float_model.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')\n",
    "print(float_model.qconfig)\n",
    "\n",
    "# train & save fp32 model on each epoch\n",
    "print(\"Training Model with QAT...\")\n",
    "quantized_model = torch.quantization.prepare_qat(float_model, inplace=False)\n",
    "quantized_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb54c48a-cc2b-4e15-b04d-f934385dc8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "print(\"Training QAT Model...\")\n",
    "quantized_model.train()\n",
    "torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = SoftTargetCrossEntropy()\n",
    "optimizer = optim.SGD(quantized_model.parameters(),\n",
    "                        lr=LR,\n",
    "                        momentum=0.9,\n",
    "                        weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                    milestones=[100, 150],\n",
    "                                                    gamma=0.1,\n",
    "                                                    last_epoch=-1)                                                  \n",
    "\n",
    "# training...                                                 \n",
    "# train_one_epoch(model=quantized_model, criterion=criterion,\n",
    "#                   data_loader=data_loader_train, optimizer=optimizer,\n",
    "#                   device=device, epoch=1, max_norm=None,\n",
    "#                   model_ema=None, mixup_fn=mixup_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3ca46",
   "metadata": {},
   "source": [
    "### Convert trained model to int8 ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cbfe0764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\ao\\quantization\\utils.py:280: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedResMLP(\n",
       "  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       "  (module): resmlp_models(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): QuantizedConv2d(3, 384, kernel_size=(16, 16), stride=(16, 16), scale=1.0, zero_point=0)\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (7): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (8): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (9): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (10): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (11): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (12): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (13): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (14): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (15): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (16): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (17): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (18): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (19): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (20): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (21): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (22): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (23): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): QuantizedLinear(in_features=384, out_features=384, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "    (head): QuantizedLinear(in_features=384, out_features=1000, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert weight to int8, replace model to quantized ver.\n",
    "quantized_model.cpu()\n",
    "torch.quantization.convert(quantized_model, inplace=True)\n",
    "quantized_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59a5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert weight to int8, replace model to quantized ver.\n",
    "quantized_model.cpu()\n",
    "torch.quantization.convert(quantized_model, inplace=True)\n",
    "quantized_model.eval()\n",
    "\n",
    "input_fp32 = torch.randn((1, 3, 224, 224), dtype=torch.float32, device=\"cpu\")\n",
    "quantized_model(input_fp32)\n",
    "\n",
    "SAVE_PATH = 'modeltest.pth'\n",
    "save_torchscript_model(model=quantized_model, \n",
    "                        model_dir='qat_weights', \n",
    "                        model_filename='qat_Test0.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b210d5bd",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d3bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1042/1042 [29:09<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: -1 Eval Loss: 1.842 Top1: 74.376 Top5: 90.640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "#model = create_model('resmlp_24', num_classes=NUM_CLASSES).cuda()\n",
    "quantized_model = load_torchscript_model(model_filepath='qat_weights/qat_Test0.pth', device=\"cpu\")\n",
    "\n",
    "#model.load_state_dict(torch.load(DICT_PATH), strict=False)\n",
    "# Evaluation\n",
    "quantized_model.eval()\n",
    "eval_loss, top1_acc, top5_acc = evaluate_model(model=quantized_model,\n",
    "                                                test_loader=data_loader_val,\n",
    "                                                device=\"cpu\",\n",
    "                                                criterion=criterion)\n",
    "print(\"Epoch: {:02d} Eval Loss: {:.3f} Top1: {:.3f} Top5: {:.3f}\".format(\n",
    "    -1, eval_loss, top1_acc, top5_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff5c0d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import run_model\n",
    "reload(run_model)\n",
    "from run_model import evaluate_model, train_one_epoch\n",
    "from run_model import load_model, save_torchscript_model, load_torchscript_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883db721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "import torch.quantization._numeric_suite as ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd6f50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = load_model(quantized_model, model_filepath='qat_weights/test.pth', device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd6ca325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedResMLP(\n",
       "  (quant): Quantize(scale=tensor([0.0358]), zero_point=tensor([125]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       "  (module): resmlp_models(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): QuantizedConv2d(3, 384, kernel_size=(16, 16), stride=(16, 16), scale=0.3109653890132904, zero_point=124)\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.073409304022789, zero_point=124, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.08399761468172073, zero_point=116, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.18273833394050598, zero_point=143, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.10778413712978363, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.3252807557582855, zero_point=133, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.015117540955543518, zero_point=109, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.19835390150547028, zero_point=137, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.2949903905391693, zero_point=124\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.13096293807029724, zero_point=106, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.403951495885849, zero_point=144, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.16631710529327393, zero_point=160, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.07191001623868942, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.11422781646251678, zero_point=131, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.06720130890607834, zero_point=117, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.06484205275774002, zero_point=131, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.14004077017307281, zero_point=123\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.05789770931005478, zero_point=135, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.11953172832727432, zero_point=110, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.20832185447216034, zero_point=193, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.04260715842247009, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.06813927739858627, zero_point=102, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.04440455138683319, zero_point=94, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.03490479290485382, zero_point=126, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.13865819573402405, zero_point=124\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.0549042634665966, zero_point=123, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.08713717013597488, zero_point=126, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.1958281695842743, zero_point=197, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.0332002155482769, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.055670496076345444, zero_point=110, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.031188011169433594, zero_point=117, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.028888868167996407, zero_point=126, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.1369871199131012, zero_point=124\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.06085407733917236, zero_point=131, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.06505176424980164, zero_point=132, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.14879603683948517, zero_point=189, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.03480939939618111, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.04482845962047577, zero_point=132, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.034784503281116486, zero_point=114, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.025506598874926567, zero_point=121, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.1346001774072647, zero_point=122\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.09563121199607849, zero_point=133, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.0682690367102623, zero_point=124, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.10920361429452896, zero_point=175, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.02764388732612133, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.06035411357879639, zero_point=158, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.047161564230918884, zero_point=127, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.028103206306695938, zero_point=133, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.1320270448923111, zero_point=123\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.12255819141864777, zero_point=128, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.0849163755774498, zero_point=130, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.09168403595685959, zero_point=148, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.027173219248652458, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.06771651655435562, zero_point=123, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.09285185486078262, zero_point=124, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.03427096828818321, zero_point=152, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.1275387406349182, zero_point=123\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (7): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.1470978707075119, zero_point=122, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.11297877132892609, zero_point=115, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.08222939819097519, zero_point=157, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.030176131054759026, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.08285515010356903, zero_point=145, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.10571824759244919, zero_point=141, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.050093621015548706, zero_point=140, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.1258772611618042, zero_point=122\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (8): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.10655774921178818, zero_point=124, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.12608623504638672, zero_point=144, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.07769273966550827, zero_point=141, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.02727775275707245, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.04968805983662605, zero_point=126, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.06873263418674469, zero_point=117, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.04360771179199219, zero_point=131, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.12214019894599915, zero_point=121\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (9): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.12162429094314575, zero_point=133, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.08159441500902176, zero_point=147, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.09914737939834595, zero_point=158, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.027957173064351082, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.046427566558122635, zero_point=103, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.05883435159921646, zero_point=133, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.04307011514902115, zero_point=137, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.11833968013525009, zero_point=122\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (10): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.11718705296516418, zero_point=125, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.0959724709391594, zero_point=156, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.07217491418123245, zero_point=132, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.029055370017886162, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.039680831134319305, zero_point=133, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.052023377269506454, zero_point=138, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.05214753374457359, zero_point=141, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.11787878721952438, zero_point=126\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (11): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.17396557331085205, zero_point=114, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.12760722637176514, zero_point=117, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.08009273558855057, zero_point=136, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.0347742885351181, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.05876484885811806, zero_point=134, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.09362504631280899, zero_point=134, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.07795833796262741, zero_point=118, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.12166230380535126, zero_point=124\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (12): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.21467158198356628, zero_point=155, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.2669355869293213, zero_point=135, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.12184278666973114, zero_point=158, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.16048207879066467, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.15138888359069824, zero_point=172, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.21202413737773895, zero_point=133, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.13182608783245087, zero_point=123, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.2251787632703781, zero_point=135\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (13): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.37029334902763367, zero_point=130, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.6011701822280884, zero_point=96, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.2720092236995697, zero_point=145, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.127436101436615, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.14889484643936157, zero_point=96, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.45547181367874146, zero_point=101, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.1883796602487564, zero_point=137, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.506277859210968, zero_point=129\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (14): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.5793780088424683, zero_point=118, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.3832968771457672, zero_point=99, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.4110559821128845, zero_point=134, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.15718437731266022, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.09512446075677872, zero_point=104, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.2863060534000397, zero_point=101, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.22223199903964996, zero_point=130, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.4891519248485565, zero_point=142\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (15): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.6339418292045593, zero_point=132, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.3304525911808014, zero_point=144, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.3997814953327179, zero_point=121, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.07194218039512634, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.14433036744594574, zero_point=120, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.29638198018074036, zero_point=105, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.2538173496723175, zero_point=130, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.5102787613868713, zero_point=125\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (16): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.8880220055580139, zero_point=140, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.4486612379550934, zero_point=90, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.4730711579322815, zero_point=93, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.12406070530414581, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.36299607157707214, zero_point=188, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.2555099129676819, zero_point=125, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.6485660076141357, zero_point=175, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.5602896809577942, zero_point=146\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (17): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.6378531455993652, zero_point=108, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.40602409839630127, zero_point=125, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.2828845977783203, zero_point=29, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.07967326045036316, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.1309877187013626, zero_point=99, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.28566375374794006, zero_point=121, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.31273168325424194, zero_point=125, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.6021257042884827, zero_point=154\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (18): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=0.7154158353805542, zero_point=124, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.4497133493423462, zero_point=121, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=1.1504672765731812, zero_point=26, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.08332314342260361, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.46965429186820984, zero_point=79, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.41812852025032043, zero_point=104, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.8609635829925537, zero_point=91, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.6555023789405823, zero_point=143\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (19): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.2151540517807007, zero_point=153, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.6029219627380371, zero_point=154, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.501205563545227, zero_point=91, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.13141576945781708, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.3976888060569763, zero_point=122, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.5168284773826599, zero_point=133, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=0.6248494982719421, zero_point=94, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=0.6984245181083679, zero_point=151\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (20): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.024816870689392, zero_point=128, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.6364981532096863, zero_point=102, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=0.6313809156417847, zero_point=150, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.07263420522212982, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=0.6429240107536316, zero_point=118, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.49173155426979065, zero_point=109, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=1.4424951076507568, zero_point=67, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.0710182189941406, zero_point=148\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (21): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=1.0779098272323608, zero_point=142, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.42221617698669434, zero_point=131, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=2.2858409881591797, zero_point=155, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.14115917682647705, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.5232070684432983, zero_point=139, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=0.39433586597442627, zero_point=144, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=7.482706069946289, zero_point=52, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=1.4806801080703735, zero_point=119\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (22): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=2.0723376274108887, zero_point=141, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=0.9426185488700867, zero_point=143, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=3.1523685455322266, zero_point=60, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.14732903242111206, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=2.4984147548675537, zero_point=77, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=1.3016527891159058, zero_point=160, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=19.486385345458984, zero_point=88, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=3.4816765785217285, zero_point=139\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (23): layers_scale_mlp_blocks(\n",
       "        (norm1): QuantizedLinear(in_features=384, out_features=384, scale=3.7688491344451904, zero_point=154, qscheme=torch.per_tensor_affine)\n",
       "        (attn): QuantizedLinear(in_features=196, out_features=196, scale=4.46009635925293, zero_point=130, qscheme=torch.per_tensor_affine)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): QuantizedLinear(in_features=384, out_features=384, scale=3.9515769481658936, zero_point=142, qscheme=torch.per_tensor_affine)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): QuantizedLinearReLU(in_features=384, out_features=1536, scale=0.11805595457553864, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "          (act): Identity()\n",
       "          (drop1): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (fc2): QuantizedLinear(in_features=1536, out_features=384, scale=1.0294075012207031, zero_point=80, qscheme=torch.per_tensor_affine)\n",
       "          (drop2): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (gamma_1): QuantizedLinear(in_features=384, out_features=384, scale=4.349467754364014, zero_point=129, qscheme=torch.per_tensor_affine)\n",
       "        (gamma_2): QuantizedLinear(in_features=384, out_features=384, scale=9.10575008392334, zero_point=117, qscheme=torch.per_tensor_affine)\n",
       "        (skip_add): QFunctional(\n",
       "          scale=3.9574480056762695, zero_point=148\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): QuantizedLinear(in_features=384, out_features=384, scale=1.5363909006118774, zero_point=134, qscheme=torch.per_tensor_affine)\n",
       "    (head): QuantizedLinear(in_features=384, out_features=1000, scale=0.05579521879553795, zero_point=72, qscheme=torch.per_tensor_affine)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab188f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys of wt_compare_dict:\n",
      "dict_keys(['module.patch_embed.proj.weight', 'module.blocks.0.norm1._packed_params._packed_params', 'module.blocks.0.attn._packed_params._packed_params', 'module.blocks.0.norm2._packed_params._packed_params', 'module.blocks.0.mlp.fc1._packed_params._packed_params', 'module.blocks.0.mlp.fc2._packed_params._packed_params', 'module.blocks.0.gamma_1._packed_params._packed_params', 'module.blocks.0.gamma_2._packed_params._packed_params', 'module.blocks.1.norm1._packed_params._packed_params', 'module.blocks.1.attn._packed_params._packed_params', 'module.blocks.1.norm2._packed_params._packed_params', 'module.blocks.1.mlp.fc1._packed_params._packed_params', 'module.blocks.1.mlp.fc2._packed_params._packed_params', 'module.blocks.1.gamma_1._packed_params._packed_params', 'module.blocks.1.gamma_2._packed_params._packed_params', 'module.blocks.2.norm1._packed_params._packed_params', 'module.blocks.2.attn._packed_params._packed_params', 'module.blocks.2.norm2._packed_params._packed_params', 'module.blocks.2.mlp.fc1._packed_params._packed_params', 'module.blocks.2.mlp.fc2._packed_params._packed_params', 'module.blocks.2.gamma_1._packed_params._packed_params', 'module.blocks.2.gamma_2._packed_params._packed_params', 'module.blocks.3.norm1._packed_params._packed_params', 'module.blocks.3.attn._packed_params._packed_params', 'module.blocks.3.norm2._packed_params._packed_params', 'module.blocks.3.mlp.fc1._packed_params._packed_params', 'module.blocks.3.mlp.fc2._packed_params._packed_params', 'module.blocks.3.gamma_1._packed_params._packed_params', 'module.blocks.3.gamma_2._packed_params._packed_params', 'module.blocks.4.norm1._packed_params._packed_params', 'module.blocks.4.attn._packed_params._packed_params', 'module.blocks.4.norm2._packed_params._packed_params', 'module.blocks.4.mlp.fc1._packed_params._packed_params', 'module.blocks.4.mlp.fc2._packed_params._packed_params', 'module.blocks.4.gamma_1._packed_params._packed_params', 'module.blocks.4.gamma_2._packed_params._packed_params', 'module.blocks.5.norm1._packed_params._packed_params', 'module.blocks.5.attn._packed_params._packed_params', 'module.blocks.5.norm2._packed_params._packed_params', 'module.blocks.5.mlp.fc1._packed_params._packed_params', 'module.blocks.5.mlp.fc2._packed_params._packed_params', 'module.blocks.5.gamma_1._packed_params._packed_params', 'module.blocks.5.gamma_2._packed_params._packed_params', 'module.blocks.6.norm1._packed_params._packed_params', 'module.blocks.6.attn._packed_params._packed_params', 'module.blocks.6.norm2._packed_params._packed_params', 'module.blocks.6.mlp.fc1._packed_params._packed_params', 'module.blocks.6.mlp.fc2._packed_params._packed_params', 'module.blocks.6.gamma_1._packed_params._packed_params', 'module.blocks.6.gamma_2._packed_params._packed_params', 'module.blocks.7.norm1._packed_params._packed_params', 'module.blocks.7.attn._packed_params._packed_params', 'module.blocks.7.norm2._packed_params._packed_params', 'module.blocks.7.mlp.fc1._packed_params._packed_params', 'module.blocks.7.mlp.fc2._packed_params._packed_params', 'module.blocks.7.gamma_1._packed_params._packed_params', 'module.blocks.7.gamma_2._packed_params._packed_params', 'module.blocks.8.norm1._packed_params._packed_params', 'module.blocks.8.attn._packed_params._packed_params', 'module.blocks.8.norm2._packed_params._packed_params', 'module.blocks.8.mlp.fc1._packed_params._packed_params', 'module.blocks.8.mlp.fc2._packed_params._packed_params', 'module.blocks.8.gamma_1._packed_params._packed_params', 'module.blocks.8.gamma_2._packed_params._packed_params', 'module.blocks.9.norm1._packed_params._packed_params', 'module.blocks.9.attn._packed_params._packed_params', 'module.blocks.9.norm2._packed_params._packed_params', 'module.blocks.9.mlp.fc1._packed_params._packed_params', 'module.blocks.9.mlp.fc2._packed_params._packed_params', 'module.blocks.9.gamma_1._packed_params._packed_params', 'module.blocks.9.gamma_2._packed_params._packed_params', 'module.blocks.10.norm1._packed_params._packed_params', 'module.blocks.10.attn._packed_params._packed_params', 'module.blocks.10.norm2._packed_params._packed_params', 'module.blocks.10.mlp.fc1._packed_params._packed_params', 'module.blocks.10.mlp.fc2._packed_params._packed_params', 'module.blocks.10.gamma_1._packed_params._packed_params', 'module.blocks.10.gamma_2._packed_params._packed_params', 'module.blocks.11.norm1._packed_params._packed_params', 'module.blocks.11.attn._packed_params._packed_params', 'module.blocks.11.norm2._packed_params._packed_params', 'module.blocks.11.mlp.fc1._packed_params._packed_params', 'module.blocks.11.mlp.fc2._packed_params._packed_params', 'module.blocks.11.gamma_1._packed_params._packed_params', 'module.blocks.11.gamma_2._packed_params._packed_params', 'module.blocks.12.norm1._packed_params._packed_params', 'module.blocks.12.attn._packed_params._packed_params', 'module.blocks.12.norm2._packed_params._packed_params', 'module.blocks.12.mlp.fc1._packed_params._packed_params', 'module.blocks.12.mlp.fc2._packed_params._packed_params', 'module.blocks.12.gamma_1._packed_params._packed_params', 'module.blocks.12.gamma_2._packed_params._packed_params', 'module.blocks.13.norm1._packed_params._packed_params', 'module.blocks.13.attn._packed_params._packed_params', 'module.blocks.13.norm2._packed_params._packed_params', 'module.blocks.13.mlp.fc1._packed_params._packed_params', 'module.blocks.13.mlp.fc2._packed_params._packed_params', 'module.blocks.13.gamma_1._packed_params._packed_params', 'module.blocks.13.gamma_2._packed_params._packed_params', 'module.blocks.14.norm1._packed_params._packed_params', 'module.blocks.14.attn._packed_params._packed_params', 'module.blocks.14.norm2._packed_params._packed_params', 'module.blocks.14.mlp.fc1._packed_params._packed_params', 'module.blocks.14.mlp.fc2._packed_params._packed_params', 'module.blocks.14.gamma_1._packed_params._packed_params', 'module.blocks.14.gamma_2._packed_params._packed_params', 'module.blocks.15.norm1._packed_params._packed_params', 'module.blocks.15.attn._packed_params._packed_params', 'module.blocks.15.norm2._packed_params._packed_params', 'module.blocks.15.mlp.fc1._packed_params._packed_params', 'module.blocks.15.mlp.fc2._packed_params._packed_params', 'module.blocks.15.gamma_1._packed_params._packed_params', 'module.blocks.15.gamma_2._packed_params._packed_params', 'module.blocks.16.norm1._packed_params._packed_params', 'module.blocks.16.attn._packed_params._packed_params', 'module.blocks.16.norm2._packed_params._packed_params', 'module.blocks.16.mlp.fc1._packed_params._packed_params', 'module.blocks.16.mlp.fc2._packed_params._packed_params', 'module.blocks.16.gamma_1._packed_params._packed_params', 'module.blocks.16.gamma_2._packed_params._packed_params', 'module.blocks.17.norm1._packed_params._packed_params', 'module.blocks.17.attn._packed_params._packed_params', 'module.blocks.17.norm2._packed_params._packed_params', 'module.blocks.17.mlp.fc1._packed_params._packed_params', 'module.blocks.17.mlp.fc2._packed_params._packed_params', 'module.blocks.17.gamma_1._packed_params._packed_params', 'module.blocks.17.gamma_2._packed_params._packed_params', 'module.blocks.18.norm1._packed_params._packed_params', 'module.blocks.18.attn._packed_params._packed_params', 'module.blocks.18.norm2._packed_params._packed_params', 'module.blocks.18.mlp.fc1._packed_params._packed_params', 'module.blocks.18.mlp.fc2._packed_params._packed_params', 'module.blocks.18.gamma_1._packed_params._packed_params', 'module.blocks.18.gamma_2._packed_params._packed_params', 'module.blocks.19.norm1._packed_params._packed_params', 'module.blocks.19.attn._packed_params._packed_params', 'module.blocks.19.norm2._packed_params._packed_params', 'module.blocks.19.mlp.fc1._packed_params._packed_params', 'module.blocks.19.mlp.fc2._packed_params._packed_params', 'module.blocks.19.gamma_1._packed_params._packed_params', 'module.blocks.19.gamma_2._packed_params._packed_params', 'module.blocks.20.norm1._packed_params._packed_params', 'module.blocks.20.attn._packed_params._packed_params', 'module.blocks.20.norm2._packed_params._packed_params', 'module.blocks.20.mlp.fc1._packed_params._packed_params', 'module.blocks.20.mlp.fc2._packed_params._packed_params', 'module.blocks.20.gamma_1._packed_params._packed_params', 'module.blocks.20.gamma_2._packed_params._packed_params', 'module.blocks.21.norm1._packed_params._packed_params', 'module.blocks.21.attn._packed_params._packed_params', 'module.blocks.21.norm2._packed_params._packed_params', 'module.blocks.21.mlp.fc1._packed_params._packed_params', 'module.blocks.21.mlp.fc2._packed_params._packed_params', 'module.blocks.21.gamma_1._packed_params._packed_params', 'module.blocks.21.gamma_2._packed_params._packed_params', 'module.blocks.22.norm1._packed_params._packed_params', 'module.blocks.22.attn._packed_params._packed_params', 'module.blocks.22.norm2._packed_params._packed_params', 'module.blocks.22.mlp.fc1._packed_params._packed_params', 'module.blocks.22.mlp.fc2._packed_params._packed_params', 'module.blocks.22.gamma_1._packed_params._packed_params', 'module.blocks.22.gamma_2._packed_params._packed_params', 'module.blocks.23.norm1._packed_params._packed_params', 'module.blocks.23.attn._packed_params._packed_params', 'module.blocks.23.norm2._packed_params._packed_params', 'module.blocks.23.mlp.fc1._packed_params._packed_params', 'module.blocks.23.mlp.fc2._packed_params._packed_params', 'module.blocks.23.gamma_1._packed_params._packed_params', 'module.blocks.23.gamma_2._packed_params._packed_params', 'module.norm._packed_params._packed_params', 'module.head._packed_params._packed_params'])\n"
     ]
    }
   ],
   "source": [
    "wt_compare_dict = ns.compare_weights(float_model.state_dict(), quantized_model.state_dict())\n",
    "print('keys of wt_compare_dict:')\n",
    "print(wt_compare_dict.keys())\n",
    "\n",
    "#print(\"\\nkeys of wt_compare_dict entry for conv1's weight:\")\n",
    "mystr = 'module.blocks.0.norm2._packed_params._packed_params'\n",
    "#print(wt_compare_dict[mystr].keys())\n",
    "#print(wt_compare_dict[mystr]['float'])\n",
    "#print(wt_compare_dict[mystr]['quantized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7c9efd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=resmlp_models\n",
       "  (patch_embed): RecursiveScriptModule(\n",
       "    original_name=PatchEmbed\n",
       "    (proj): RecursiveScriptModule(original_name=Conv2d)\n",
       "    (norm): RecursiveScriptModule(original_name=Identity)\n",
       "  )\n",
       "  (blocks): RecursiveScriptModule(\n",
       "    original_name=ModuleList\n",
       "    (0): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (1): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (2): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (3): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (4): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (5): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (6): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (7): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (8): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (9): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (10): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (11): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (12): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (13): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (14): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (15): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (16): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (17): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (18): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (19): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (20): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (21): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (22): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "    (23): RecursiveScriptModule(\n",
       "      original_name=layers_scale_mlp_blocks\n",
       "      (norm1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (attn): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (drop_path): RecursiveScriptModule(original_name=Identity)\n",
       "      (norm2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (mlp): RecursiveScriptModule(\n",
       "        original_name=Mlp\n",
       "        (fc1): RecursiveScriptModule(\n",
       "          original_name=LinearReLU\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (act): RecursiveScriptModule(original_name=Identity)\n",
       "        (drop1): RecursiveScriptModule(original_name=Dropout)\n",
       "        (fc2): RecursiveScriptModule(\n",
       "          original_name=Linear\n",
       "          (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "        )\n",
       "        (drop2): RecursiveScriptModule(original_name=Dropout)\n",
       "      )\n",
       "      (gamma_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (gamma_2): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "      )\n",
       "      (skip_add): RecursiveScriptModule(\n",
       "        original_name=QFunctional\n",
       "        (activation_post_process): RecursiveScriptModule(original_name=Identity)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): RecursiveScriptModule(\n",
       "    original_name=Linear\n",
       "    (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "  )\n",
       "  (head): RecursiveScriptModule(\n",
       "    original_name=Linear\n",
       "    (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3f141aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, val in quantized_model.module.state_dict().items():\n",
    "  print(f\"{name}:\", end=\" \")\n",
    "  s = val.shape\n",
    "  print(f\"\\t{s}\")\n",
    "\n",
    "# print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "babd9276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('patch_embed.proj.weight',\n",
       "              tensor([[[[ 1.8547e-02,  3.1375e-02, -3.4824e-04,  ...,  1.0358e-02,\n",
       "                         -2.4780e-02, -3.0519e-02],\n",
       "                        [-6.4273e-03, -1.6747e-02,  3.3060e-03,  ...,  2.3723e-02,\n",
       "                         -9.9016e-03,  1.8857e-02],\n",
       "                        [-2.0897e-02,  3.2766e-02, -7.3521e-04,  ..., -3.0972e-02,\n",
       "                          1.9634e-02,  3.2349e-02],\n",
       "                        ...,\n",
       "                        [ 2.4877e-02,  1.8711e-02,  6.0188e-03,  ..., -1.8882e-02,\n",
       "                          5.2874e-03, -2.7802e-02],\n",
       "                        [-1.2207e-02,  1.5945e-02,  1.8371e-02,  ...,  1.2325e-03,\n",
       "                         -2.8527e-02, -2.4299e-02],\n",
       "                        [ 8.1906e-03, -1.7239e-02,  3.3641e-03,  ...,  3.0933e-02,\n",
       "                          2.2067e-02,  1.3474e-02]],\n",
       "              \n",
       "                       [[ 1.2587e-02,  2.6582e-02,  6.8919e-03,  ...,  8.6741e-03,\n",
       "                          5.6368e-03,  7.6251e-03],\n",
       "                        [ 2.7883e-02,  1.4649e-02,  2.9489e-02,  ..., -1.5535e-02,\n",
       "                          1.3183e-02,  3.4733e-02],\n",
       "                        [-4.5538e-03,  1.9167e-02,  3.9423e-03,  ...,  1.8928e-03,\n",
       "                         -3.9341e-03, -2.0761e-02],\n",
       "                        ...,\n",
       "                        [ 1.8591e-02, -1.8961e-02,  2.4194e-02,  ..., -3.2903e-02,\n",
       "                          3.4405e-02,  1.5061e-02],\n",
       "                        [ 2.4205e-02, -1.0977e-02, -6.4084e-03,  ..., -2.5262e-02,\n",
       "                          3.0551e-02,  2.5967e-02],\n",
       "                        [-1.4229e-02,  2.8722e-02,  2.6449e-02,  ..., -2.8582e-02,\n",
       "                          8.0151e-03,  6.8139e-03]],\n",
       "              \n",
       "                       [[ 6.6099e-03, -1.3269e-02, -1.5422e-02,  ..., -3.3254e-02,\n",
       "                         -1.9298e-02, -1.9690e-02],\n",
       "                        [-2.4713e-02, -3.1622e-02,  3.4573e-03,  ...,  2.6780e-03,\n",
       "                          1.9341e-02,  3.3312e-02],\n",
       "                        [ 1.3259e-02, -3.1914e-03, -4.0131e-04,  ..., -2.7130e-02,\n",
       "                         -8.4962e-03, -2.3524e-02],\n",
       "                        ...,\n",
       "                        [-1.3656e-02, -2.9955e-02,  1.1527e-02,  ...,  1.7662e-02,\n",
       "                          2.1927e-03,  1.9747e-02],\n",
       "                        [-1.9652e-02, -3.0710e-02,  1.6558e-02,  ...,  9.1202e-03,\n",
       "                          8.8299e-03,  7.6964e-03],\n",
       "                        [ 2.2587e-02, -2.5698e-02, -2.3077e-02,  ...,  1.1553e-02,\n",
       "                          3.8196e-03,  6.2305e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.8069e-03, -2.6590e-02, -8.1506e-03,  ..., -3.0814e-02,\n",
       "                          6.1451e-03, -1.7567e-02],\n",
       "                        [ 2.0718e-02, -3.1152e-02, -2.5361e-03,  ...,  3.0378e-02,\n",
       "                          3.6895e-04,  6.5429e-03],\n",
       "                        [ 3.4565e-02,  1.6634e-02, -2.4648e-02,  ..., -6.5388e-03,\n",
       "                         -2.7775e-02,  5.5961e-03],\n",
       "                        ...,\n",
       "                        [-3.5735e-02,  2.6475e-02, -2.4902e-03,  ..., -6.9625e-03,\n",
       "                          2.5297e-02,  2.8383e-02],\n",
       "                        [-4.9975e-03, -3.3125e-02, -2.7379e-02,  ...,  1.9615e-02,\n",
       "                         -1.9399e-02, -6.8145e-03],\n",
       "                        [ 1.4015e-02,  2.8664e-02,  2.5157e-02,  ..., -2.6568e-04,\n",
       "                          2.0985e-03,  3.4195e-02]],\n",
       "              \n",
       "                       [[ 2.9363e-02,  1.4065e-02,  3.2163e-02,  ..., -2.5900e-02,\n",
       "                         -2.2792e-02, -3.2818e-02],\n",
       "                        [ 2.5747e-02, -2.4916e-02,  2.6693e-02,  ..., -2.7881e-02,\n",
       "                         -2.0753e-02,  4.2934e-03],\n",
       "                        [ 6.3985e-03, -1.9718e-02,  1.1480e-02,  ..., -2.9824e-02,\n",
       "                         -3.2052e-02,  3.1778e-02],\n",
       "                        ...,\n",
       "                        [ 1.0481e-02,  7.1462e-03,  2.5591e-02,  ..., -1.7017e-02,\n",
       "                          8.6883e-03,  7.5539e-03],\n",
       "                        [-1.8717e-02,  1.7116e-02, -2.9019e-02,  ...,  2.8611e-02,\n",
       "                          4.8876e-03, -7.5423e-03],\n",
       "                        [-3.4427e-02, -6.8756e-03,  1.5219e-02,  ...,  9.8487e-03,\n",
       "                         -1.4583e-02,  1.1635e-02]],\n",
       "              \n",
       "                       [[ 1.5520e-02, -3.0729e-02, -9.3798e-03,  ..., -3.3357e-02,\n",
       "                         -1.7178e-02,  1.6330e-02],\n",
       "                        [-1.8127e-02,  2.6785e-02,  3.3604e-02,  ..., -3.0625e-02,\n",
       "                         -2.1292e-02, -2.7717e-02],\n",
       "                        [ 3.2035e-03, -3.8755e-03,  3.5371e-02,  ..., -2.0548e-02,\n",
       "                          3.0016e-02,  2.0822e-02],\n",
       "                        ...,\n",
       "                        [-2.7417e-02, -1.1426e-02, -4.8744e-03,  ..., -1.9100e-02,\n",
       "                          1.3934e-02, -2.4990e-02],\n",
       "                        [ 2.4033e-02, -2.0918e-02, -2.7357e-02,  ..., -1.0407e-02,\n",
       "                          6.4505e-03, -3.0551e-02],\n",
       "                        [ 9.9475e-03,  1.2339e-02, -7.2446e-03,  ...,  1.8997e-02,\n",
       "                         -7.0517e-03,  3.2577e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1424e-02, -2.6476e-02, -1.9004e-02,  ..., -1.5624e-02,\n",
       "                          1.6926e-02, -1.7606e-02],\n",
       "                        [ 1.3886e-02,  1.2541e-02,  1.6954e-02,  ...,  8.8989e-03,\n",
       "                          2.7767e-02, -3.4974e-02],\n",
       "                        [-8.6257e-03, -1.1852e-02, -2.8456e-02,  ...,  3.4536e-02,\n",
       "                         -1.4667e-02, -1.3744e-02],\n",
       "                        ...,\n",
       "                        [-8.2057e-05, -1.0409e-02, -1.3915e-02,  ..., -1.7384e-02,\n",
       "                         -2.2092e-02,  1.6004e-02],\n",
       "                        [ 3.2846e-02, -5.5035e-03,  2.4739e-02,  ..., -2.5502e-02,\n",
       "                         -1.0417e-02,  1.4826e-02],\n",
       "                        [-3.3813e-02,  2.5599e-02, -2.1868e-02,  ..., -2.6651e-03,\n",
       "                         -3.8461e-03, -5.5542e-03]],\n",
       "              \n",
       "                       [[ 3.0223e-02,  1.6119e-02, -4.1252e-03,  ..., -3.4999e-02,\n",
       "                          2.6326e-02,  1.1076e-02],\n",
       "                        [ 1.4527e-02, -1.9416e-02, -3.3852e-02,  ...,  1.2269e-02,\n",
       "                          2.8875e-02,  1.9679e-03],\n",
       "                        [ 2.3854e-02, -3.5245e-02,  2.2674e-02,  ...,  9.9912e-03,\n",
       "                         -2.6948e-02,  2.6843e-02],\n",
       "                        ...,\n",
       "                        [ 3.3253e-02,  3.2634e-03, -2.1012e-02,  ...,  3.0908e-02,\n",
       "                          1.2663e-02,  1.7639e-02],\n",
       "                        [ 3.1955e-02,  2.3001e-02,  2.4097e-02,  ...,  2.4679e-02,\n",
       "                         -2.8172e-02,  2.8718e-02],\n",
       "                        [-1.0226e-02,  2.5262e-02, -2.9356e-02,  ..., -9.3637e-03,\n",
       "                          3.2205e-02, -6.9891e-03]],\n",
       "              \n",
       "                       [[ 2.5541e-02,  2.5904e-02, -2.3092e-02,  ...,  2.1608e-03,\n",
       "                          4.1455e-03, -2.9553e-02],\n",
       "                        [-2.3604e-02, -3.2482e-02, -5.4450e-03,  ...,  2.7915e-02,\n",
       "                         -3.1292e-02,  2.8969e-02],\n",
       "                        [ 1.8843e-02,  1.0900e-02, -2.3807e-04,  ..., -2.6097e-02,\n",
       "                         -3.0928e-03, -2.3647e-02],\n",
       "                        ...,\n",
       "                        [ 2.4928e-02,  9.2168e-03, -3.0302e-02,  ...,  1.8189e-02,\n",
       "                         -3.6018e-02,  3.2496e-02],\n",
       "                        [-2.1073e-02,  1.9949e-02,  3.5957e-02,  ...,  4.5534e-03,\n",
       "                         -1.7475e-02,  1.4172e-02],\n",
       "                        [-2.1850e-02,  5.7197e-03,  1.9519e-02,  ...,  1.7252e-02,\n",
       "                          1.4779e-04,  1.8524e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-2.7706e-02, -5.7542e-03,  2.1180e-02,  ..., -1.8650e-02,\n",
       "                         -1.1094e-02, -2.1425e-03],\n",
       "                        [ 7.1149e-03, -8.6344e-03,  1.6210e-02,  ...,  3.0075e-02,\n",
       "                         -1.7680e-02,  1.7167e-02],\n",
       "                        [-2.6072e-02, -2.1211e-02, -2.7563e-02,  ...,  8.7543e-03,\n",
       "                          9.7911e-03,  4.1263e-03],\n",
       "                        ...,\n",
       "                        [-4.9326e-03,  3.4897e-02, -2.5471e-02,  ..., -2.2624e-02,\n",
       "                          2.5589e-02, -3.0268e-02],\n",
       "                        [-6.2163e-03, -1.5839e-02, -2.6027e-02,  ...,  1.2883e-02,\n",
       "                          3.2192e-02, -1.2991e-02],\n",
       "                        [ 3.3514e-02, -2.2354e-02,  1.2930e-02,  ...,  1.6290e-02,\n",
       "                         -3.2023e-02,  2.7851e-02]],\n",
       "              \n",
       "                       [[ 1.7556e-02, -1.2316e-02, -7.8622e-03,  ..., -4.0559e-03,\n",
       "                          6.2300e-03, -5.3292e-03],\n",
       "                        [-1.2023e-03,  6.1462e-03, -1.4148e-02,  ..., -2.6580e-02,\n",
       "                         -2.5380e-02,  7.5281e-04],\n",
       "                        [ 3.4914e-02,  2.8680e-02,  2.6080e-02,  ...,  3.5040e-02,\n",
       "                          3.2572e-03,  3.5006e-02],\n",
       "                        ...,\n",
       "                        [ 1.6890e-02, -1.7423e-02,  1.7350e-02,  ...,  5.1287e-03,\n",
       "                          1.2344e-02, -2.4737e-02],\n",
       "                        [ 5.1107e-03, -3.5835e-02,  1.8777e-02,  ..., -7.5388e-03,\n",
       "                          7.0814e-05,  2.4116e-02],\n",
       "                        [ 1.5171e-02,  3.1311e-02, -3.6123e-03,  ...,  2.5581e-02,\n",
       "                         -3.1848e-02,  1.8548e-02]],\n",
       "              \n",
       "                       [[-3.3729e-02,  2.9092e-02, -1.3896e-02,  ..., -2.3829e-02,\n",
       "                         -2.3267e-02,  2.2157e-02],\n",
       "                        [ 1.4716e-03, -2.5409e-02,  2.4149e-02,  ..., -9.8616e-04,\n",
       "                          3.0924e-02,  2.2902e-02],\n",
       "                        [-3.0636e-02,  1.3849e-02,  1.7984e-02,  ..., -2.8041e-02,\n",
       "                          3.5993e-02,  1.7652e-02],\n",
       "                        ...,\n",
       "                        [ 1.6293e-02,  7.1149e-03,  1.5235e-02,  ...,  1.6429e-02,\n",
       "                          2.3798e-02,  1.4630e-02],\n",
       "                        [-1.6964e-02, -2.0711e-02,  1.6960e-02,  ...,  1.2407e-02,\n",
       "                         -1.5057e-02, -1.3527e-02],\n",
       "                        [-2.2965e-02,  5.9679e-03, -2.1317e-02,  ...,  1.5158e-02,\n",
       "                          8.0228e-03, -2.3410e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3176e-02, -2.0281e-02,  1.3253e-02,  ..., -2.2537e-02,\n",
       "                          2.8377e-02, -9.0160e-03],\n",
       "                        [-3.0286e-02,  4.1409e-03, -3.1487e-02,  ..., -1.3271e-02,\n",
       "                         -2.4017e-02, -1.1772e-02],\n",
       "                        [-4.0368e-04,  3.6031e-02,  1.3416e-02,  ..., -7.4647e-03,\n",
       "                          1.2969e-02,  3.1128e-02],\n",
       "                        ...,\n",
       "                        [-2.0249e-02, -2.9842e-02,  9.4307e-03,  ..., -2.1938e-02,\n",
       "                         -9.3881e-03, -3.0692e-02],\n",
       "                        [-3.4050e-03,  1.2189e-02,  2.0113e-03,  ..., -2.6107e-03,\n",
       "                          3.6058e-02, -1.1003e-02],\n",
       "                        [ 2.7283e-02, -9.5566e-03,  1.0647e-02,  ...,  2.4654e-02,\n",
       "                         -1.6473e-02, -2.1548e-02]],\n",
       "              \n",
       "                       [[ 7.2043e-03, -2.9629e-02, -1.9455e-02,  ..., -2.5048e-02,\n",
       "                          3.2175e-02,  1.4243e-02],\n",
       "                        [-3.2547e-02, -1.3750e-02, -3.4238e-02,  ...,  2.4639e-02,\n",
       "                         -3.4736e-02,  6.5107e-03],\n",
       "                        [-6.0545e-03, -2.6731e-02,  2.3718e-02,  ...,  6.9974e-04,\n",
       "                          2.6492e-02, -2.5198e-02],\n",
       "                        ...,\n",
       "                        [-2.9933e-02,  5.0440e-03,  5.6336e-03,  ...,  2.1319e-03,\n",
       "                         -2.6242e-03,  2.5938e-02],\n",
       "                        [ 2.5075e-02, -8.8657e-03,  1.4298e-02,  ...,  2.6561e-02,\n",
       "                          2.1732e-03, -5.2605e-03],\n",
       "                        [ 3.3045e-02,  2.1210e-02, -8.9037e-03,  ..., -8.2664e-03,\n",
       "                         -2.8970e-02, -3.5791e-02]],\n",
       "              \n",
       "                       [[ 2.5424e-02, -1.0797e-02, -1.1507e-02,  ..., -2.3159e-02,\n",
       "                         -1.5132e-03, -1.4574e-02],\n",
       "                        [-3.0903e-02, -1.3394e-02,  2.1164e-02,  ...,  2.6553e-02,\n",
       "                         -2.7488e-02,  2.6190e-02],\n",
       "                        [ 2.8153e-02,  2.0686e-02, -8.4451e-03,  ...,  1.0919e-02,\n",
       "                          2.6308e-02, -5.2547e-03],\n",
       "                        ...,\n",
       "                        [-1.1709e-02,  3.4285e-02, -1.5451e-02,  ..., -2.7427e-02,\n",
       "                         -3.4981e-02,  2.1490e-02],\n",
       "                        [-1.0015e-02,  2.4340e-03, -4.3050e-03,  ...,  3.2941e-02,\n",
       "                          2.6105e-02, -3.1961e-02],\n",
       "                        [ 1.3396e-02, -1.6794e-02, -6.1364e-04,  ...,  3.4457e-02,\n",
       "                         -1.0189e-03, -2.7841e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.1812e-03, -1.7272e-02, -5.0535e-03,  ..., -3.5832e-02,\n",
       "                         -9.6814e-04,  1.1848e-02],\n",
       "                        [ 2.9020e-02,  3.3901e-02, -2.9435e-02,  ...,  2.4963e-02,\n",
       "                         -3.1537e-02, -3.1792e-02],\n",
       "                        [ 1.9960e-02,  1.1562e-02, -8.4160e-03,  ...,  5.9475e-03,\n",
       "                          1.5482e-02,  2.8067e-03],\n",
       "                        ...,\n",
       "                        [ 1.1487e-02,  8.1400e-03,  3.2838e-02,  ...,  1.8703e-02,\n",
       "                          2.1079e-03, -1.8937e-02],\n",
       "                        [-2.1471e-02,  2.6560e-02, -3.5957e-02,  ..., -2.4038e-02,\n",
       "                          1.7630e-03, -1.3085e-02],\n",
       "                        [ 6.4642e-03, -2.6560e-02,  1.7089e-02,  ...,  2.2307e-03,\n",
       "                          1.4644e-02,  3.2982e-02]],\n",
       "              \n",
       "                       [[ 1.3675e-02,  2.7877e-02,  2.7149e-02,  ..., -8.6839e-03,\n",
       "                          3.0274e-02,  3.3776e-02],\n",
       "                        [-2.2216e-02, -1.9989e-02,  1.5815e-02,  ..., -2.5028e-02,\n",
       "                         -4.5565e-03, -3.0273e-02],\n",
       "                        [ 6.4199e-04, -3.1071e-02, -9.0092e-03,  ..., -5.4015e-03,\n",
       "                          1.1443e-02,  1.7022e-03],\n",
       "                        ...,\n",
       "                        [ 8.9240e-03, -2.9959e-02, -1.7664e-02,  ..., -1.9645e-02,\n",
       "                          1.1003e-02,  2.2956e-02],\n",
       "                        [ 1.6692e-02, -7.1877e-04, -1.5021e-02,  ..., -2.3523e-03,\n",
       "                         -3.6026e-02,  3.4795e-02],\n",
       "                        [-2.4191e-02,  1.5516e-03, -3.1823e-03,  ...,  2.4539e-02,\n",
       "                          8.7574e-03, -1.7614e-02]],\n",
       "              \n",
       "                       [[-2.7974e-02,  3.7941e-03, -1.4652e-02,  ..., -1.2498e-02,\n",
       "                         -6.0407e-03, -2.4968e-02],\n",
       "                        [ 2.3855e-02,  7.1290e-03,  1.4880e-02,  ...,  1.9399e-02,\n",
       "                          2.5290e-02, -3.3164e-02],\n",
       "                        [-9.7735e-03,  1.3705e-02, -7.1231e-03,  ...,  6.5363e-03,\n",
       "                         -1.5862e-02,  8.7718e-03],\n",
       "                        ...,\n",
       "                        [ 1.6383e-02, -1.3834e-02, -1.6012e-02,  ...,  2.9385e-02,\n",
       "                         -2.4444e-02,  1.3920e-02],\n",
       "                        [ 8.7522e-04, -8.0102e-03, -1.1628e-03,  ..., -2.1608e-03,\n",
       "                         -1.7377e-02, -2.3574e-02],\n",
       "                        [ 1.3227e-02,  1.2918e-02, -5.4617e-03,  ..., -1.0293e-02,\n",
       "                         -2.7958e-02, -1.4967e-02]]]], device='cuda:0')),\n",
       "             ('patch_embed.proj.bias',\n",
       "              tensor([ 2.6556e-02,  2.3717e-02, -5.9157e-03,  3.9601e-03,  5.4708e-03,\n",
       "                       1.0896e-02, -1.2325e-02, -3.4602e-02, -2.2663e-02, -1.9540e-02,\n",
       "                      -2.5646e-02, -9.7340e-03,  3.4310e-02, -2.8856e-02,  3.2019e-02,\n",
       "                       1.4784e-02,  3.4739e-02, -6.5888e-03, -2.3632e-02,  3.8876e-03,\n",
       "                       1.2395e-03,  1.4214e-02,  2.6788e-02, -2.7019e-02,  3.3182e-02,\n",
       "                      -2.0066e-02,  2.4548e-02,  1.8513e-02, -5.4062e-03, -2.5169e-02,\n",
       "                       3.0524e-02, -7.7484e-03,  1.3230e-03, -2.0812e-02, -2.9157e-02,\n",
       "                       7.2800e-03, -1.7502e-02,  3.3104e-02, -1.2214e-02, -2.1015e-02,\n",
       "                      -1.9513e-02, -9.1803e-03, -2.2624e-02,  3.5498e-02,  5.2953e-03,\n",
       "                       2.1794e-02, -2.5951e-02, -4.4822e-03,  1.2870e-02,  1.1346e-02,\n",
       "                      -2.9864e-02, -1.5934e-02, -2.9329e-02,  3.5973e-02,  5.2863e-03,\n",
       "                      -2.1976e-03,  2.8150e-02, -3.5290e-02, -6.3766e-03, -3.1950e-02,\n",
       "                       3.1155e-02,  3.1666e-02, -8.1769e-03,  1.5975e-02,  1.0375e-02,\n",
       "                       3.0074e-03,  1.8740e-02, -1.8537e-02,  2.8986e-02, -6.1333e-03,\n",
       "                       3.5108e-02, -7.8722e-03,  1.8510e-02,  2.4313e-02, -6.8817e-03,\n",
       "                      -1.5258e-02,  6.8069e-03, -1.6591e-02, -2.3911e-02,  2.9339e-02,\n",
       "                      -1.0700e-02, -3.4240e-02, -2.4443e-02,  3.2307e-03, -7.0714e-03,\n",
       "                       1.2582e-02, -1.4152e-02,  2.4167e-02,  2.5988e-02,  1.9252e-02,\n",
       "                       2.8529e-02, -2.1622e-02,  1.7533e-02, -3.5770e-02,  4.2430e-03,\n",
       "                       7.8183e-03,  1.0365e-02, -6.6544e-03, -2.3889e-03, -2.9317e-02,\n",
       "                      -2.1690e-02,  8.7013e-04,  2.6303e-02, -6.8836e-03, -5.7036e-03,\n",
       "                       3.4369e-02,  2.6992e-02,  4.1887e-03,  1.9380e-02,  1.1418e-02,\n",
       "                      -3.2932e-02, -2.0832e-02, -2.2265e-02,  4.8199e-03,  3.3450e-02,\n",
       "                       2.9300e-04,  5.3558e-03, -2.3499e-02,  1.9992e-02,  8.8928e-03,\n",
       "                       3.0280e-02,  1.2155e-02, -1.0516e-02, -3.4196e-02, -2.7157e-02,\n",
       "                      -1.2457e-02, -1.0846e-02,  1.1768e-02, -1.1590e-02,  2.5383e-02,\n",
       "                      -1.8045e-02, -2.9850e-03, -3.1514e-02, -2.2940e-02, -2.4978e-02,\n",
       "                      -2.1178e-02, -5.8598e-03, -2.4983e-02, -7.9609e-04,  9.5004e-03,\n",
       "                      -2.5057e-02, -3.1837e-02, -3.0501e-02, -2.8846e-02,  2.8312e-02,\n",
       "                      -1.0338e-02, -3.3860e-02,  3.0691e-02,  2.6066e-02,  6.5665e-03,\n",
       "                      -2.7309e-02,  9.1206e-03, -3.3499e-02,  5.7627e-03,  2.5469e-02,\n",
       "                      -1.3553e-02, -6.4850e-04,  3.5278e-02, -2.1817e-02,  2.6268e-02,\n",
       "                       1.4256e-02,  3.2976e-02, -2.7818e-02, -3.5445e-04,  1.4896e-02,\n",
       "                      -2.7746e-02,  1.9475e-02, -3.0154e-02, -2.1534e-02,  1.5501e-02,\n",
       "                       4.1708e-03, -8.8469e-03,  1.8734e-02, -1.3972e-02,  3.5998e-02,\n",
       "                       3.0661e-02, -3.3450e-02,  1.8163e-02, -9.0712e-03, -1.7191e-02,\n",
       "                       3.3670e-02,  1.0046e-02,  2.1811e-02, -1.9910e-02,  2.0935e-02,\n",
       "                       6.1578e-03, -3.2927e-02,  6.0489e-03,  6.9870e-03, -1.3139e-02,\n",
       "                       3.0510e-03, -3.0504e-02, -2.3274e-02,  3.0938e-03,  7.8622e-04,\n",
       "                      -2.9372e-03,  2.2201e-02,  3.1529e-03, -1.7802e-02,  2.8361e-02,\n",
       "                      -2.1944e-02,  2.4599e-02,  3.2620e-02, -3.2771e-02, -1.6385e-02,\n",
       "                       1.9399e-02,  2.3326e-02,  9.1587e-04, -3.4722e-02, -1.6932e-02,\n",
       "                      -2.7136e-02,  2.2916e-03,  2.7237e-02, -2.9152e-02, -3.8771e-03,\n",
       "                      -3.0168e-03, -6.4803e-03, -2.3423e-02, -1.2685e-02,  3.1602e-02,\n",
       "                      -2.9137e-02, -6.7079e-03,  3.3444e-02, -1.2784e-02,  3.1443e-02,\n",
       "                      -3.5403e-02,  2.6051e-02,  2.4424e-02, -2.0271e-02,  3.3060e-02,\n",
       "                       5.5508e-03,  1.5591e-02, -3.1929e-02,  2.7062e-02,  1.5105e-02,\n",
       "                       8.4651e-03,  2.5564e-02,  2.1688e-02, -2.1967e-02,  9.1706e-03,\n",
       "                      -1.6901e-02,  2.8213e-02, -3.2019e-02,  5.0332e-03,  1.0079e-02,\n",
       "                      -3.0790e-02,  6.3869e-03, -6.3667e-03,  2.3108e-02, -3.0827e-02,\n",
       "                      -3.3079e-02, -9.4329e-03,  1.2265e-02,  2.3926e-02,  3.4410e-02,\n",
       "                       1.6298e-02, -2.9169e-02, -2.3510e-02,  1.1257e-02, -6.3995e-03,\n",
       "                      -1.7355e-02,  2.2377e-02, -1.9784e-02, -2.9246e-02, -2.5952e-02,\n",
       "                      -1.3670e-02, -9.0100e-03,  2.8036e-02, -3.0741e-02,  1.8739e-02,\n",
       "                       3.5790e-02,  1.0627e-02,  2.5779e-02, -2.0432e-02, -3.5976e-02,\n",
       "                      -1.0667e-02, -2.7528e-02,  1.7082e-02,  2.5325e-02,  1.6000e-02,\n",
       "                       1.0598e-02,  3.0034e-02, -2.2179e-02,  2.6389e-02, -1.5475e-02,\n",
       "                       3.3438e-02,  6.1714e-03,  3.2772e-02,  3.2011e-02,  1.6171e-02,\n",
       "                       1.2702e-02,  2.2774e-02, -2.0106e-03,  9.6741e-03, -1.4511e-03,\n",
       "                       1.3773e-02, -2.5002e-02,  3.4509e-03,  1.3655e-02,  1.0091e-02,\n",
       "                      -3.3061e-02,  3.5670e-02,  1.3059e-02, -1.3934e-03,  1.5570e-02,\n",
       "                      -1.2877e-02, -1.4824e-02,  9.6128e-03,  2.7186e-02,  9.6605e-03,\n",
       "                      -2.0809e-02,  2.6121e-02,  2.7111e-02, -1.0313e-02,  1.3868e-02,\n",
       "                      -1.4524e-04, -3.2224e-02, -9.0914e-03,  2.0056e-02,  4.3083e-05,\n",
       "                       2.6585e-02,  1.8042e-02, -8.1483e-03,  1.3073e-02, -3.0641e-02,\n",
       "                      -4.1877e-03, -8.8222e-03,  3.2497e-02, -3.6031e-02,  2.0626e-02,\n",
       "                       7.4050e-03, -1.8105e-02,  1.9096e-02, -2.6014e-02,  2.1262e-02,\n",
       "                      -2.3175e-02, -2.8439e-02, -1.9868e-02, -1.2154e-02, -2.0948e-02,\n",
       "                       1.5302e-02, -2.3206e-02,  2.4874e-02,  2.0439e-02, -2.2883e-02,\n",
       "                      -3.1859e-02, -1.3213e-02, -2.3106e-02,  2.0892e-02,  2.8135e-03,\n",
       "                       3.3181e-03, -4.0697e-03,  2.2915e-02, -2.9706e-02, -1.3679e-02,\n",
       "                       2.4575e-02,  2.9325e-02, -3.2927e-02, -2.7615e-02,  3.0913e-02,\n",
       "                      -2.7351e-02,  2.3228e-02,  1.4381e-02,  1.0902e-02, -5.2592e-03,\n",
       "                      -3.4363e-02,  2.6678e-02, -3.0213e-03,  3.3390e-02,  1.5374e-02,\n",
       "                       1.7712e-02, -4.2935e-03, -3.0053e-02, -2.0046e-02, -2.1599e-02,\n",
       "                      -2.3662e-03, -2.7677e-02, -1.7309e-02, -1.4470e-02,  2.1004e-02,\n",
       "                       4.0847e-03, -2.4734e-02, -1.3322e-02,  2.5712e-03], device='cuda:0')),\n",
       "             ('blocks.0.norm1.weight',\n",
       "              tensor([[-0.0053,  0.0253, -0.0052,  ...,  0.0158, -0.0029, -0.0326],\n",
       "                      [-0.0084,  0.0038, -0.0091,  ...,  0.0090,  0.0111, -0.0181],\n",
       "                      [ 0.0467, -0.0105,  0.0036,  ...,  0.0280,  0.0002,  0.0048],\n",
       "                      ...,\n",
       "                      [ 0.0004,  0.0168,  0.0311,  ..., -0.0164, -0.0023,  0.0133],\n",
       "                      [ 0.0100, -0.0115,  0.0056,  ...,  0.0047,  0.0089, -0.0051],\n",
       "                      [-0.0246, -0.0135,  0.0501,  ...,  0.0148,  0.0081, -0.0124]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.attn.weight',\n",
       "              tensor([[-0.0080,  0.0267,  0.0326,  ...,  0.0347,  0.0351, -0.0230],\n",
       "                      [-0.0136, -0.0087,  0.0070,  ...,  0.0006, -0.0344,  0.0091],\n",
       "                      [-0.0122, -0.0012, -0.0214,  ..., -0.0224, -0.0169,  0.0100],\n",
       "                      ...,\n",
       "                      [-0.0213,  0.0040, -0.0056,  ..., -0.0196, -0.0211,  0.0068],\n",
       "                      [-0.0036, -0.0102, -0.0064,  ..., -0.0218, -0.0378,  0.0169],\n",
       "                      [-0.0104,  0.0119, -0.0131,  ...,  0.0374, -0.0101,  0.0151]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.0.norm2.weight',\n",
       "              tensor([[-0.0265, -0.0175, -0.0114,  ..., -0.0062,  0.0158,  0.0049],\n",
       "                      [ 0.0060, -0.0068,  0.0152,  ...,  0.0301, -0.0084,  0.0094],\n",
       "                      [-0.0168, -0.0197, -0.0033,  ...,  0.0213,  0.0129, -0.0284],\n",
       "                      ...,\n",
       "                      [ 0.0213,  0.0037, -0.0068,  ..., -0.0075, -0.0189, -0.0021],\n",
       "                      [ 0.0078, -0.0010, -0.0295,  ...,  0.0037, -0.0186,  0.0042],\n",
       "                      [-0.0059,  0.0278, -0.0170,  ..., -0.0081, -0.0042, -0.0064]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.mlp.fc1.weight',\n",
       "              tensor([[-7.5000e-03, -8.2716e-03, -1.0713e-02,  ..., -1.5803e-02,\n",
       "                        1.0711e-02,  1.5395e-02],\n",
       "                      [-1.1531e-02,  1.6704e-02, -2.3251e-02,  ..., -2.5262e-03,\n",
       "                        1.0535e-02,  1.4823e-02],\n",
       "                      [ 2.3485e-02,  6.0647e-04,  1.1462e-02,  ..., -8.1779e-03,\n",
       "                       -1.8140e-02,  2.9589e-02],\n",
       "                      ...,\n",
       "                      [ 2.2827e-03,  1.1023e-02,  6.3680e-05,  ...,  2.2964e-03,\n",
       "                       -1.5463e-03, -2.7278e-02],\n",
       "                      [-8.0296e-03,  7.7418e-03, -1.0416e-02,  ..., -1.9281e-02,\n",
       "                       -1.3086e-03, -5.1840e-03],\n",
       "                      [-1.9172e-02,  8.3357e-03,  5.5829e-03,  ..., -4.7994e-02,\n",
       "                       -2.3134e-02, -2.5992e-03]], device='cuda:0')),\n",
       "             ('blocks.0.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.0.mlp.fc2.weight',\n",
       "              tensor([[ 0.0327,  0.0277, -0.0286,  ...,  0.0113, -0.0181,  0.0174],\n",
       "                      [ 0.0012,  0.0155, -0.0181,  ...,  0.0027,  0.0100,  0.0260],\n",
       "                      [-0.0151,  0.0033,  0.0140,  ...,  0.0210, -0.0181,  0.0031],\n",
       "                      ...,\n",
       "                      [-0.0245, -0.0278,  0.0118,  ..., -0.0319,  0.0090, -0.0161],\n",
       "                      [-0.0465, -0.0172, -0.0325,  ...,  0.0303, -0.0172, -0.0077],\n",
       "                      [ 0.0231,  0.0218, -0.0078,  ...,  0.0071,  0.0326, -0.0024]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.gamma_1.weight',\n",
       "              tensor([[-0.0122, -0.0135,  0.0238,  ..., -0.0135,  0.0127, -0.0055],\n",
       "                      [ 0.0352,  0.0099,  0.0247,  ..., -0.0135, -0.0212,  0.0025],\n",
       "                      [-0.0052,  0.0284, -0.0677,  ..., -0.0009, -0.0017,  0.0153],\n",
       "                      ...,\n",
       "                      [-0.0130,  0.0262,  0.0040,  ..., -0.0063,  0.0180,  0.0056],\n",
       "                      [-0.0085, -0.0120, -0.0029,  ...,  0.0282, -0.0035, -0.0025],\n",
       "                      [-0.0102, -0.0692,  0.0179,  ...,  0.0141, -0.0328, -0.0023]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.gamma_2.weight',\n",
       "              tensor([[-0.0102, -0.0245,  0.0075,  ..., -0.0204, -0.0101, -0.0077],\n",
       "                      [-0.0034,  0.0159,  0.0252,  ...,  0.0091,  0.0165, -0.0031],\n",
       "                      [-0.0149, -0.0256, -0.0061,  ...,  0.0112,  0.0103,  0.0135],\n",
       "                      ...,\n",
       "                      [ 0.0220, -0.0034, -0.0171,  ...,  0.0079, -0.0105,  0.0170],\n",
       "                      [ 0.0407,  0.0047, -0.0049,  ..., -0.0170,  0.0060, -0.0010],\n",
       "                      [ 0.0038, -0.0168, -0.0240,  ...,  0.0116,  0.0268,  0.0107]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.norm1.weight',\n",
       "              tensor([[-0.0336,  0.0203,  0.0016,  ..., -0.0074, -0.0255, -0.0020],\n",
       "                      [-0.0174, -0.0018, -0.0068,  ...,  0.0237, -0.0166, -0.0232],\n",
       "                      [ 0.0267, -0.0277,  0.0276,  ...,  0.0229, -0.0045,  0.0120],\n",
       "                      ...,\n",
       "                      [ 0.0148,  0.0198,  0.0195,  ...,  0.0095, -0.0295, -0.0288],\n",
       "                      [ 0.0043, -0.0083,  0.0137,  ...,  0.0017,  0.0023,  0.0034],\n",
       "                      [-0.0143,  0.0085, -0.0039,  ...,  0.0111, -0.0138,  0.0234]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.attn.weight',\n",
       "              tensor([[ 0.0105, -0.0135,  0.0056,  ...,  0.0014,  0.0146,  0.0105],\n",
       "                      [-0.0099,  0.0038, -0.0134,  ..., -0.0339,  0.0159,  0.0127],\n",
       "                      [-0.0022, -0.0071,  0.0073,  ..., -0.0357, -0.0005,  0.0094],\n",
       "                      ...,\n",
       "                      [-0.0475, -0.0151, -0.0224,  ...,  0.0115,  0.0078,  0.0269],\n",
       "                      [ 0.0292, -0.0130,  0.0286,  ...,  0.0488,  0.0389,  0.0114],\n",
       "                      [-0.0144, -0.0050, -0.0398,  ...,  0.0061, -0.0058, -0.0041]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.1.norm2.weight',\n",
       "              tensor([[-0.0014,  0.0189, -0.0026,  ..., -0.0047,  0.0134,  0.0004],\n",
       "                      [ 0.0183,  0.0082, -0.0096,  ..., -0.0268, -0.0196, -0.0133],\n",
       "                      [-0.0032,  0.0104, -0.0174,  ...,  0.0189, -0.0330,  0.0380],\n",
       "                      ...,\n",
       "                      [ 0.0229,  0.0223, -0.0081,  ...,  0.0085, -0.0342,  0.0221],\n",
       "                      [-0.0089,  0.0279,  0.0043,  ..., -0.0295, -0.0088, -0.0215],\n",
       "                      [ 0.0138, -0.0301,  0.0050,  ...,  0.0289, -0.0117, -0.0018]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.mlp.fc1.weight',\n",
       "              tensor([[-0.0051, -0.0201, -0.0236,  ..., -0.0122, -0.0011,  0.0186],\n",
       "                      [ 0.0063, -0.0079, -0.0111,  ..., -0.0235,  0.0228, -0.0039],\n",
       "                      [-0.0133,  0.0034,  0.0231,  ..., -0.0259, -0.0096,  0.0075],\n",
       "                      ...,\n",
       "                      [-0.0113, -0.0014,  0.0006,  ..., -0.0154, -0.0179,  0.0283],\n",
       "                      [-0.0118,  0.0209,  0.0112,  ..., -0.0263,  0.0143,  0.0303],\n",
       "                      [-0.0171, -0.0078, -0.0141,  ...,  0.0008,  0.0271, -0.0012]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.1.mlp.fc2.weight',\n",
       "              tensor([[ 0.0161,  0.0155,  0.0342,  ...,  0.0351, -0.0143,  0.0149],\n",
       "                      [ 0.0220, -0.0037,  0.0122,  ..., -0.0154, -0.0035, -0.0380],\n",
       "                      [ 0.0017,  0.0143, -0.0110,  ..., -0.0197,  0.0034, -0.0025],\n",
       "                      ...,\n",
       "                      [ 0.0212, -0.0093,  0.0003,  ...,  0.0043, -0.0250, -0.0167],\n",
       "                      [-0.0039, -0.0474, -0.0023,  ..., -0.0075,  0.0125,  0.0069],\n",
       "                      [-0.0103,  0.0065,  0.0227,  ..., -0.0135,  0.0205, -0.0050]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.gamma_1.weight',\n",
       "              tensor([[ 0.0043, -0.0046, -0.0012,  ..., -0.0053, -0.0104,  0.0112],\n",
       "                      [-0.0149, -0.0068,  0.0346,  ...,  0.0036, -0.0143,  0.0102],\n",
       "                      [ 0.0137,  0.0307,  0.0167,  ...,  0.0229, -0.0441,  0.0122],\n",
       "                      ...,\n",
       "                      [-0.0069, -0.0527,  0.0033,  ...,  0.0005,  0.0068, -0.0288],\n",
       "                      [ 0.0267, -0.0086,  0.0115,  ...,  0.0289,  0.0131, -0.0227],\n",
       "                      [-0.0307, -0.0076, -0.0143,  ..., -0.0071, -0.0020,  0.0204]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.gamma_2.weight',\n",
       "              tensor([[-0.0114, -0.0200, -0.0151,  ...,  0.0085, -0.0072,  0.0260],\n",
       "                      [-0.0093, -0.0141, -0.0075,  ...,  0.0095,  0.0192, -0.0032],\n",
       "                      [ 0.0018, -0.0029,  0.0151,  ..., -0.0085, -0.0086,  0.0173],\n",
       "                      ...,\n",
       "                      [ 0.0056, -0.0273, -0.0067,  ...,  0.0130, -0.0084, -0.0041],\n",
       "                      [ 0.0149, -0.0193,  0.0079,  ..., -0.0261,  0.0300, -0.0242],\n",
       "                      [-0.0036, -0.0213,  0.0208,  ...,  0.0029,  0.0099,  0.0114]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.norm1.weight',\n",
       "              tensor([[ 0.0150, -0.0067,  0.0188,  ...,  0.0227, -0.0126,  0.0009],\n",
       "                      [ 0.0014,  0.0074,  0.0184,  ..., -0.0082,  0.0166,  0.0066],\n",
       "                      [ 0.0153, -0.0016,  0.0100,  ...,  0.0236, -0.0289, -0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0094, -0.0048,  0.0366,  ...,  0.0211,  0.0105,  0.0377],\n",
       "                      [ 0.0307,  0.0215, -0.0125,  ...,  0.0030, -0.0315, -0.0293],\n",
       "                      [-0.0113, -0.0068,  0.0084,  ..., -0.0135, -0.0373,  0.0322]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.attn.weight',\n",
       "              tensor([[ 0.0076,  0.0094, -0.0191,  ...,  0.0069,  0.0327,  0.0043],\n",
       "                      [ 0.0063, -0.0198,  0.0002,  ..., -0.0410,  0.0086, -0.0134],\n",
       "                      [ 0.0199, -0.0208,  0.0207,  ..., -0.0132, -0.0441, -0.0197],\n",
       "                      ...,\n",
       "                      [-0.0046, -0.0145,  0.0226,  ...,  0.0356,  0.0066,  0.0042],\n",
       "                      [ 0.0067,  0.0334,  0.0153,  ..., -0.0138, -0.0379, -0.0023],\n",
       "                      [ 0.0097, -0.0105,  0.0108,  ...,  0.0107,  0.0069, -0.0021]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.2.norm2.weight',\n",
       "              tensor([[ 0.0037, -0.0492,  0.0230,  ...,  0.0039, -0.0027,  0.0214],\n",
       "                      [-0.0086,  0.0158, -0.0004,  ...,  0.0013, -0.0110,  0.0100],\n",
       "                      [-0.0202,  0.0150, -0.0418,  ...,  0.0145, -0.0388, -0.0161],\n",
       "                      ...,\n",
       "                      [ 0.0192,  0.0155, -0.0041,  ...,  0.0008, -0.0147,  0.0271],\n",
       "                      [ 0.0025, -0.0012,  0.0043,  ...,  0.0145, -0.0058, -0.0079],\n",
       "                      [ 0.0328,  0.0113,  0.0213,  ...,  0.0349,  0.0215, -0.0014]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.mlp.fc1.weight',\n",
       "              tensor([[ 0.0079,  0.0152,  0.0197,  ..., -0.0015, -0.0057,  0.0201],\n",
       "                      [-0.0015, -0.0130,  0.0096,  ..., -0.0094, -0.0043,  0.0303],\n",
       "                      [ 0.0049,  0.0574,  0.0060,  ...,  0.0193,  0.0081,  0.0499],\n",
       "                      ...,\n",
       "                      [-0.0191,  0.0059, -0.0381,  ...,  0.0050,  0.0132,  0.0017],\n",
       "                      [-0.0009, -0.0164, -0.0438,  ..., -0.0196, -0.0600, -0.0523],\n",
       "                      [ 0.0139,  0.0177,  0.0094,  ..., -0.0022,  0.0428, -0.0017]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.2.mlp.fc2.weight',\n",
       "              tensor([[ 0.0037,  0.0078, -0.0222,  ..., -0.0083,  0.0097,  0.0479],\n",
       "                      [ 0.0023, -0.0155, -0.0105,  ...,  0.0142, -0.0084, -0.0047],\n",
       "                      [ 0.0084, -0.0124, -0.0203,  ..., -0.0129, -0.0100, -0.0029],\n",
       "                      ...,\n",
       "                      [ 0.0213, -0.0044,  0.0236,  ..., -0.0006, -0.0209, -0.0007],\n",
       "                      [ 0.0112, -0.0201, -0.0086,  ..., -0.0346,  0.0116, -0.0204],\n",
       "                      [-0.0166, -0.0204,  0.0331,  ...,  0.0174, -0.0059,  0.0202]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.gamma_1.weight',\n",
       "              tensor([[-0.0190,  0.0175,  0.0325,  ...,  0.0306,  0.0062,  0.0100],\n",
       "                      [-0.0170,  0.0020, -0.0107,  ...,  0.0197, -0.0064, -0.0237],\n",
       "                      [ 0.0237, -0.0261,  0.0133,  ..., -0.0122, -0.0093, -0.0074],\n",
       "                      ...,\n",
       "                      [-0.0078,  0.0120, -0.0088,  ..., -0.0072,  0.0106,  0.0124],\n",
       "                      [ 0.0023, -0.0159,  0.0312,  ...,  0.0545, -0.0287, -0.0177],\n",
       "                      [ 0.0380, -0.0083,  0.0048,  ...,  0.0051, -0.0129, -0.0097]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.gamma_2.weight',\n",
       "              tensor([[-2.9598e-04, -2.6551e-03, -4.0553e-02,  ...,  3.0385e-02,\n",
       "                       -1.7207e-02,  2.7545e-02],\n",
       "                      [ 6.0293e-04,  1.6084e-03,  4.0831e-02,  ...,  2.2773e-02,\n",
       "                       -2.2243e-03, -4.3776e-02],\n",
       "                      [ 2.4122e-02,  1.2866e-02,  3.4241e-02,  ..., -3.9064e-02,\n",
       "                       -7.0413e-05,  9.6142e-03],\n",
       "                      ...,\n",
       "                      [-3.3209e-02, -3.2754e-02,  1.5168e-02,  ..., -1.5214e-02,\n",
       "                       -2.3097e-02,  3.9411e-02],\n",
       "                      [ 1.3802e-02,  3.2862e-02, -7.2657e-03,  ...,  2.6551e-02,\n",
       "                       -4.5722e-04,  1.1182e-03],\n",
       "                      [-2.5019e-03,  1.8969e-03,  2.5529e-02,  ...,  1.6904e-03,\n",
       "                        1.1775e-02,  1.7232e-02]], device='cuda:0')),\n",
       "             ('blocks.3.norm1.weight',\n",
       "              tensor([[-0.0003,  0.0192,  0.0166,  ..., -0.0115,  0.0043,  0.0073],\n",
       "                      [ 0.0250,  0.0466, -0.0081,  ...,  0.0084, -0.0028,  0.0141],\n",
       "                      [-0.0057, -0.0145, -0.0367,  ..., -0.0209, -0.0303,  0.0035],\n",
       "                      ...,\n",
       "                      [ 0.0432,  0.0177, -0.0192,  ...,  0.0207,  0.0274, -0.0043],\n",
       "                      [-0.0076, -0.0224,  0.0089,  ...,  0.0287, -0.0125,  0.0006],\n",
       "                      [-0.0146, -0.0059, -0.0266,  ...,  0.0267, -0.0028,  0.0147]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.attn.weight',\n",
       "              tensor([[-0.0148, -0.0409, -0.0079,  ...,  0.0075, -0.0027, -0.0284],\n",
       "                      [ 0.0121, -0.0012,  0.0042,  ...,  0.0040, -0.0090, -0.0102],\n",
       "                      [ 0.0068, -0.0036, -0.0019,  ...,  0.0037, -0.0006, -0.0144],\n",
       "                      ...,\n",
       "                      [ 0.0328,  0.0078,  0.0004,  ...,  0.0014,  0.0104,  0.0387],\n",
       "                      [ 0.0189,  0.0150, -0.0037,  ..., -0.0329, -0.0105,  0.0163],\n",
       "                      [ 0.0105,  0.0050, -0.0211,  ..., -0.0278,  0.0494, -0.0201]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.3.norm2.weight',\n",
       "              tensor([[-0.0030, -0.0107, -0.0037,  ...,  0.0189, -0.0070,  0.0232],\n",
       "                      [ 0.0208, -0.0264,  0.0126,  ..., -0.0034, -0.0297,  0.0439],\n",
       "                      [ 0.0135, -0.0031, -0.0014,  ..., -0.0109, -0.0254, -0.0347],\n",
       "                      ...,\n",
       "                      [ 0.0002,  0.0121,  0.0235,  ..., -0.0073, -0.0163, -0.0034],\n",
       "                      [-0.0097,  0.0050,  0.0012,  ..., -0.0082, -0.0120, -0.0051],\n",
       "                      [-0.0030,  0.0073, -0.0196,  ..., -0.0051,  0.0003, -0.0324]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.mlp.fc1.weight',\n",
       "              tensor([[ 0.0122,  0.0080, -0.0159,  ...,  0.0036, -0.0043,  0.0222],\n",
       "                      [-0.0065, -0.0167,  0.0165,  ...,  0.0186,  0.0054, -0.0024],\n",
       "                      [ 0.0097,  0.0087, -0.0230,  ...,  0.0301,  0.0129,  0.0215],\n",
       "                      ...,\n",
       "                      [ 0.0156,  0.0003, -0.0193,  ...,  0.0365, -0.0057,  0.0072],\n",
       "                      [-0.0122,  0.0199, -0.0296,  ...,  0.0269,  0.0085,  0.0245],\n",
       "                      [ 0.0344,  0.0007,  0.0170,  ..., -0.0070,  0.0157,  0.0009]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.3.mlp.fc2.weight',\n",
       "              tensor([[ 1.5034e-02, -8.3872e-03,  1.7275e-03,  ..., -3.4347e-03,\n",
       "                       -5.1181e-03,  3.2081e-05],\n",
       "                      [ 1.7770e-02,  2.6517e-02,  6.5780e-03,  ..., -3.2352e-02,\n",
       "                       -9.2923e-03, -4.7089e-04],\n",
       "                      [ 1.7070e-03,  7.2585e-03, -1.8788e-02,  ..., -2.2905e-02,\n",
       "                        4.1586e-03, -2.8624e-02],\n",
       "                      ...,\n",
       "                      [-5.3034e-03, -2.0485e-02, -5.2942e-02,  ..., -2.4363e-02,\n",
       "                        1.0676e-03, -3.6998e-03],\n",
       "                      [-4.4176e-03,  2.5617e-02,  1.0591e-02,  ...,  9.7624e-03,\n",
       "                        2.6569e-02,  3.8076e-02],\n",
       "                      [-1.3755e-02, -5.6608e-03, -4.1595e-03,  ..., -1.1419e-02,\n",
       "                        1.3983e-02, -1.9018e-02]], device='cuda:0')),\n",
       "             ('blocks.3.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.gamma_1.weight',\n",
       "              tensor([[-0.0563, -0.0132, -0.0147,  ..., -0.0086, -0.0024, -0.0109],\n",
       "                      [ 0.0211, -0.0012,  0.0025,  ..., -0.0031, -0.0352, -0.0251],\n",
       "                      [ 0.0006,  0.0050,  0.0293,  ..., -0.0252, -0.0110, -0.0172],\n",
       "                      ...,\n",
       "                      [ 0.0132,  0.0390,  0.0086,  ..., -0.0102,  0.0396, -0.0075],\n",
       "                      [ 0.0116, -0.0198, -0.0056,  ...,  0.0194, -0.0138, -0.0095],\n",
       "                      [-0.0181, -0.0093,  0.0107,  ...,  0.0163, -0.0029, -0.0085]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.gamma_2.weight',\n",
       "              tensor([[ 0.0184,  0.0234,  0.0254,  ..., -0.0022, -0.0167, -0.0208],\n",
       "                      [-0.0226,  0.0043, -0.0185,  ..., -0.0025,  0.0051,  0.0333],\n",
       "                      [-0.0225, -0.0134,  0.0160,  ...,  0.0152, -0.0157, -0.0283],\n",
       "                      ...,\n",
       "                      [ 0.0278,  0.0297,  0.0090,  ...,  0.0132, -0.0251, -0.0108],\n",
       "                      [-0.0055, -0.0183,  0.0071,  ...,  0.0044, -0.0220,  0.0073],\n",
       "                      [ 0.0100,  0.0054, -0.0003,  ...,  0.0010, -0.0277,  0.0133]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.norm1.weight',\n",
       "              tensor([[-0.0424, -0.0129, -0.0110,  ..., -0.0328, -0.0032, -0.0367],\n",
       "                      [ 0.0308, -0.0211,  0.0193,  ..., -0.0233,  0.0235,  0.0273],\n",
       "                      [ 0.0332,  0.0110,  0.0335,  ..., -0.0119,  0.0473,  0.0037],\n",
       "                      ...,\n",
       "                      [ 0.0033, -0.0036, -0.0161,  ..., -0.0038, -0.0018,  0.0025],\n",
       "                      [-0.0031,  0.0427, -0.0220,  ...,  0.0041,  0.0198,  0.0151],\n",
       "                      [-0.0185, -0.0044,  0.0327,  ...,  0.0027,  0.0052, -0.0168]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.attn.weight',\n",
       "              tensor([[ 0.0099, -0.0044,  0.0081,  ..., -0.0179, -0.0150,  0.0442],\n",
       "                      [-0.0025, -0.0034,  0.0109,  ...,  0.0081, -0.0471,  0.0035],\n",
       "                      [-0.0151, -0.0027, -0.0088,  ...,  0.0322,  0.0166,  0.0119],\n",
       "                      ...,\n",
       "                      [ 0.0350,  0.0121,  0.0187,  ..., -0.0058, -0.0058, -0.0185],\n",
       "                      [-0.0039, -0.0086, -0.0189,  ..., -0.0132, -0.0097,  0.0069],\n",
       "                      [ 0.0264,  0.0550,  0.0214,  ...,  0.0295,  0.0308, -0.0003]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.4.norm2.weight',\n",
       "              tensor([[ 0.0066,  0.0081, -0.0001,  ..., -0.0107, -0.0140,  0.0055],\n",
       "                      [-0.0096, -0.0721, -0.0198,  ...,  0.0145,  0.0159, -0.0021],\n",
       "                      [-0.0052, -0.0040, -0.0216,  ..., -0.0139, -0.0059,  0.0127],\n",
       "                      ...,\n",
       "                      [ 0.0197, -0.0310,  0.0208,  ..., -0.0013,  0.0075, -0.0118],\n",
       "                      [-0.0230, -0.0111, -0.0161,  ...,  0.0037, -0.0243,  0.0216],\n",
       "                      [ 0.0031, -0.0399,  0.0189,  ..., -0.0088,  0.0064, -0.0069]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.mlp.fc1.weight',\n",
       "              tensor([[-0.0014, -0.0161, -0.0037,  ..., -0.0103, -0.0046,  0.0159],\n",
       "                      [-0.0265,  0.0127, -0.0257,  ..., -0.0103, -0.0084, -0.0056],\n",
       "                      [-0.0051, -0.0054, -0.0269,  ...,  0.0145,  0.0067,  0.0145],\n",
       "                      ...,\n",
       "                      [-0.0321, -0.0004,  0.0087,  ..., -0.0205, -0.0178,  0.0252],\n",
       "                      [-0.0142, -0.0665, -0.0217,  ...,  0.0112,  0.0206, -0.0106],\n",
       "                      [ 0.0414, -0.0304, -0.0075,  ...,  0.0330, -0.0202,  0.0293]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.4.mlp.fc2.weight',\n",
       "              tensor([[-0.0093, -0.0465,  0.0138,  ...,  0.0194, -0.0234, -0.0420],\n",
       "                      [-0.0303,  0.0014, -0.0333,  ..., -0.0086,  0.0120,  0.0113],\n",
       "                      [-0.0043,  0.0005,  0.0058,  ..., -0.0373, -0.0051,  0.0052],\n",
       "                      ...,\n",
       "                      [-0.0188, -0.0083,  0.0246,  ..., -0.0133, -0.0308, -0.0125],\n",
       "                      [ 0.0113, -0.0160, -0.0081,  ...,  0.0043, -0.0305,  0.0037],\n",
       "                      [ 0.0338,  0.0097, -0.0322,  ...,  0.0030, -0.0151, -0.0145]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.gamma_1.weight',\n",
       "              tensor([[ 0.0250,  0.0040, -0.0377,  ...,  0.0212, -0.0233, -0.0033],\n",
       "                      [ 0.0127,  0.0082, -0.0167,  ..., -0.0299,  0.0128, -0.0024],\n",
       "                      [ 0.0070,  0.0189, -0.0214,  ...,  0.0077, -0.0241, -0.0159],\n",
       "                      ...,\n",
       "                      [ 0.0047, -0.0017,  0.0339,  ...,  0.0012, -0.0067, -0.0152],\n",
       "                      [ 0.0011,  0.0110, -0.0031,  ..., -0.0041, -0.0102,  0.0089],\n",
       "                      [ 0.0034,  0.0315,  0.0234,  ..., -0.0386, -0.0083,  0.0132]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.gamma_2.weight',\n",
       "              tensor([[-0.0150, -0.0136, -0.0005,  ..., -0.0314, -0.0103, -0.0150],\n",
       "                      [-0.0398, -0.0184,  0.0015,  ...,  0.0260,  0.0326, -0.0209],\n",
       "                      [-0.0122,  0.0129, -0.0226,  ..., -0.0048, -0.0271,  0.0057],\n",
       "                      ...,\n",
       "                      [-0.0255,  0.0073, -0.0249,  ..., -0.0086,  0.0387, -0.0355],\n",
       "                      [ 0.0189, -0.0082,  0.0046,  ...,  0.0242, -0.0126,  0.0429],\n",
       "                      [-0.0073, -0.0254,  0.0191,  ...,  0.0020, -0.0065,  0.0100]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.norm1.weight',\n",
       "              tensor([[ 0.0496, -0.0003,  0.0241,  ..., -0.0006, -0.0154, -0.0003],\n",
       "                      [-0.0258,  0.0141,  0.0207,  ..., -0.0308, -0.0196,  0.0204],\n",
       "                      [ 0.0216, -0.0024,  0.0094,  ..., -0.0027, -0.0320,  0.0058],\n",
       "                      ...,\n",
       "                      [ 0.0051, -0.0219, -0.0021,  ...,  0.0042, -0.0055, -0.0024],\n",
       "                      [-0.0155,  0.0238,  0.0043,  ..., -0.0137,  0.0215,  0.0169],\n",
       "                      [ 0.0002,  0.0081,  0.0061,  ...,  0.0083, -0.0034,  0.0053]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.attn.weight',\n",
       "              tensor([[-0.0059, -0.0260,  0.0053,  ...,  0.0076,  0.0173,  0.0214],\n",
       "                      [-0.0262,  0.0063, -0.0200,  ..., -0.0017, -0.0180,  0.0259],\n",
       "                      [-0.0164,  0.0332,  0.0363,  ...,  0.0011,  0.0118, -0.0020],\n",
       "                      ...,\n",
       "                      [ 0.0292, -0.0158, -0.0330,  ..., -0.0045, -0.0049,  0.0082],\n",
       "                      [-0.0211,  0.0004,  0.0028,  ...,  0.0173,  0.0026, -0.0396],\n",
       "                      [ 0.0113, -0.0136, -0.0134,  ...,  0.0061,  0.0095, -0.0276]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.5.norm2.weight',\n",
       "              tensor([[ 4.7216e-05,  1.8473e-02,  6.0509e-03,  ...,  1.3436e-02,\n",
       "                       -1.2772e-02, -3.0966e-02],\n",
       "                      [ 4.5016e-03,  1.0888e-02,  1.3049e-02,  ...,  1.8024e-03,\n",
       "                        1.2253e-02,  2.7862e-02],\n",
       "                      [-4.7258e-03, -2.4356e-02, -4.4422e-03,  ..., -1.1024e-03,\n",
       "                        1.0122e-02,  1.5754e-03],\n",
       "                      ...,\n",
       "                      [ 2.1101e-03, -3.2863e-02,  3.6818e-03,  ..., -2.7455e-02,\n",
       "                        8.2452e-05, -2.2494e-02],\n",
       "                      [-2.2450e-02, -3.1947e-02, -5.0793e-03,  ..., -5.1333e-04,\n",
       "                        3.8719e-03, -1.8293e-03],\n",
       "                      [ 8.6575e-03, -1.2227e-02,  2.7594e-02,  ..., -1.0104e-02,\n",
       "                        5.2986e-04, -3.1830e-02]], device='cuda:0')),\n",
       "             ('blocks.5.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.mlp.fc1.weight',\n",
       "              tensor([[ 0.0630,  0.0131, -0.0022,  ...,  0.0070, -0.0130, -0.0307],\n",
       "                      [ 0.0183,  0.0406,  0.0003,  ...,  0.0224,  0.0137, -0.0355],\n",
       "                      [-0.0042, -0.0037, -0.0177,  ..., -0.0441, -0.0162, -0.0122],\n",
       "                      ...,\n",
       "                      [-0.0516,  0.0182, -0.0277,  ..., -0.0345, -0.0210,  0.0044],\n",
       "                      [-0.0239, -0.0096, -0.0279,  ..., -0.0395,  0.0099, -0.0217],\n",
       "                      [-0.0154,  0.0067,  0.0306,  ..., -0.0192,  0.0074, -0.0346]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.5.mlp.fc2.weight',\n",
       "              tensor([[ 5.0219e-03,  8.3644e-04, -1.7758e-05,  ...,  3.3389e-02,\n",
       "                        7.9239e-03, -2.0485e-02],\n",
       "                      [-2.2266e-02, -1.6543e-02, -7.2869e-03,  ...,  1.4645e-02,\n",
       "                       -2.0219e-02, -2.3146e-02],\n",
       "                      [ 2.7703e-03, -1.9655e-03,  9.8341e-03,  ...,  7.0030e-03,\n",
       "                        2.9571e-02, -2.6096e-02],\n",
       "                      ...,\n",
       "                      [-5.1436e-03,  5.0094e-02, -1.7630e-02,  ...,  8.7313e-03,\n",
       "                       -2.1834e-02, -3.2592e-03],\n",
       "                      [-8.2964e-03,  6.3526e-03, -3.9835e-03,  ..., -9.8460e-03,\n",
       "                        7.2555e-03, -1.8887e-02],\n",
       "                      [ 6.1842e-03,  9.3079e-03,  2.8487e-03,  ...,  1.9431e-02,\n",
       "                       -3.9172e-03, -8.3304e-03]], device='cuda:0')),\n",
       "             ('blocks.5.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.gamma_1.weight',\n",
       "              tensor([[-0.0161, -0.0213,  0.0100,  ..., -0.0075,  0.0136,  0.0074],\n",
       "                      [ 0.0108, -0.0191, -0.0074,  ...,  0.0193,  0.0360,  0.0170],\n",
       "                      [-0.0241, -0.0134,  0.0112,  ..., -0.0039, -0.0147,  0.0136],\n",
       "                      ...,\n",
       "                      [-0.0306,  0.0121,  0.0327,  ...,  0.0548, -0.0154, -0.0100],\n",
       "                      [ 0.0050, -0.0172, -0.0381,  ...,  0.0126, -0.0179,  0.0102],\n",
       "                      [-0.0082, -0.0203, -0.0004,  ...,  0.0037,  0.0197,  0.0367]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.gamma_2.weight',\n",
       "              tensor([[-0.0001,  0.0183, -0.0098,  ...,  0.0007,  0.0251,  0.0006],\n",
       "                      [ 0.0235,  0.0085, -0.0318,  ...,  0.0015,  0.0308,  0.0075],\n",
       "                      [-0.0074, -0.0520, -0.0197,  ..., -0.0009,  0.0137, -0.0159],\n",
       "                      ...,\n",
       "                      [ 0.0029,  0.0259,  0.0196,  ..., -0.0287,  0.0015,  0.0042],\n",
       "                      [ 0.0009,  0.0253, -0.0050,  ..., -0.0174, -0.0032, -0.0055],\n",
       "                      [ 0.0100, -0.0136,  0.0034,  ..., -0.0029,  0.0409,  0.0225]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.6.norm1.weight',\n",
       "              tensor([[ 0.0227,  0.0390, -0.0131,  ..., -0.0255, -0.0123,  0.0038],\n",
       "                      [ 0.0049,  0.0219, -0.0189,  ...,  0.0094,  0.0010,  0.0188],\n",
       "                      [-0.0154, -0.0084,  0.0101,  ...,  0.0019, -0.0444,  0.0138],\n",
       "                      ...,\n",
       "                      [-0.0127, -0.0159, -0.0175,  ...,  0.0013, -0.0014,  0.0068],\n",
       "                      [-0.0049, -0.0127,  0.0262,  ..., -0.0049, -0.0402, -0.0092],\n",
       "                      [-0.0091,  0.0057,  0.0088,  ..., -0.0254,  0.0014, -0.0155]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.6.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.6.attn.weight',\n",
       "              tensor([[ 0.0436, -0.0056, -0.0353,  ..., -0.0257, -0.0109,  0.0478],\n",
       "                      [ 0.0010, -0.0099,  0.0015,  ...,  0.0344,  0.0450,  0.0273],\n",
       "                      [-0.0310, -0.0038, -0.0167,  ...,  0.0077,  0.0014,  0.0167],\n",
       "                      ...,\n",
       "                      [ 0.0162, -0.0132, -0.0210,  ..., -0.0147,  0.0057,  0.0008],\n",
       "                      [ 0.0093, -0.0565,  0.0030,  ...,  0.0099, -0.0184, -0.0174],\n",
       "                      [-0.0173,  0.0315,  0.0370,  ..., -0.0038,  0.0076, -0.0127]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.6.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.6.norm2.weight',\n",
       "              tensor([[-0.0009, -0.0069,  0.0073,  ..., -0.0116,  0.0131,  0.0003],\n",
       "                      [ 0.0126, -0.0287,  0.0065,  ...,  0.0291, -0.0067,  0.0257],\n",
       "                      [-0.0003,  0.0406, -0.0180,  ..., -0.0338,  0.0055,  0.0174],\n",
       "                      ...,\n",
       "                      [ 0.0027,  0.0330, -0.0019,  ...,  0.0285,  0.0386,  0.0163],\n",
       "                      [ 0.0131, -0.0129,  0.0045,  ...,  0.0017,  0.0357,  0.0060],\n",
       "                      [ 0.0176,  0.0140, -0.0087,  ..., -0.0288,  0.0071,  0.0142]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.6.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.6.mlp.fc1.weight',\n",
       "              tensor([[-0.0011,  0.0036,  0.0120,  ...,  0.0144, -0.0290,  0.0142],\n",
       "                      [-0.0200, -0.0057, -0.0080,  ...,  0.0054, -0.0210, -0.0043],\n",
       "                      [ 0.0217, -0.0175, -0.0168,  ..., -0.0119, -0.0006,  0.0091],\n",
       "                      ...,\n",
       "                      [-0.0134,  0.0219,  0.0085,  ..., -0.0073,  0.0175, -0.0220],\n",
       "                      [ 0.0009,  0.0073, -0.0205,  ...,  0.0145, -0.0010, -0.0242],\n",
       "                      [ 0.0280, -0.0155,  0.0066,  ..., -0.0308, -0.0175, -0.0203]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.6.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.6.mlp.fc2.weight',\n",
       "              tensor([[-0.0243, -0.0299,  0.0149,  ..., -0.0309,  0.0183,  0.0110],\n",
       "                      [-0.0013, -0.0382, -0.0076,  ..., -0.0382,  0.0086, -0.0141],\n",
       "                      [ 0.0079, -0.0099, -0.0206,  ..., -0.0098,  0.0349,  0.0010],\n",
       "                      ...,\n",
       "                      [-0.0236,  0.0154, -0.0122,  ..., -0.0124,  0.0179, -0.0117],\n",
       "                      [-0.0163,  0.0121,  0.0050,  ..., -0.0134, -0.0217,  0.0155],\n",
       "                      [ 0.0178, -0.0084, -0.0057,  ...,  0.0083, -0.0176, -0.0086]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.6.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.6.gamma_1.weight',\n",
       "              tensor([[-0.0002, -0.0043,  0.0174,  ..., -0.0090,  0.0387,  0.0220],\n",
       "                      [ 0.0123, -0.0265, -0.0522,  ..., -0.0188, -0.0219, -0.0335],\n",
       "                      [-0.0225,  0.0056,  0.0283,  ...,  0.0170, -0.0012, -0.0356],\n",
       "                      ...,\n",
       "                      [ 0.0015,  0.0067, -0.0038,  ...,  0.0053, -0.0009, -0.0092],\n",
       "                      [-0.0236,  0.0031, -0.0051,  ...,  0.0046,  0.0108, -0.0161],\n",
       "                      [ 0.0300,  0.0089,  0.0339,  ..., -0.0118, -0.0151, -0.0371]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.6.gamma_2.weight',\n",
       "              tensor([[-0.0169,  0.0025, -0.0277,  ..., -0.0332, -0.0018,  0.0178],\n",
       "                      [-0.0138, -0.0002,  0.0029,  ...,  0.0252, -0.0131,  0.0238],\n",
       "                      [-0.0025, -0.0196, -0.0151,  ...,  0.0192,  0.0331, -0.0033],\n",
       "                      ...,\n",
       "                      [-0.0062,  0.0184, -0.0095,  ..., -0.0195, -0.0333,  0.0096],\n",
       "                      [ 0.0013, -0.0096, -0.0226,  ..., -0.0041,  0.0048,  0.0183],\n",
       "                      [ 0.0078, -0.0164,  0.0232,  ...,  0.0205,  0.0040,  0.0023]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.7.norm1.weight',\n",
       "              tensor([[ 0.0014,  0.0228,  0.0020,  ...,  0.0096, -0.0064, -0.0157],\n",
       "                      [-0.0006, -0.0015, -0.0161,  ..., -0.0620, -0.0096, -0.0036],\n",
       "                      [-0.0186, -0.0063,  0.0117,  ..., -0.0050, -0.0049, -0.0232],\n",
       "                      ...,\n",
       "                      [ 0.0036, -0.0308,  0.0183,  ..., -0.0328,  0.0069, -0.0109],\n",
       "                      [-0.0206, -0.0081, -0.0122,  ...,  0.0105, -0.0122, -0.0198],\n",
       "                      [-0.0284,  0.0115,  0.0134,  ...,  0.0052,  0.0077, -0.0133]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.7.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.7.attn.weight',\n",
       "              tensor([[-0.0001,  0.0115, -0.0221,  ..., -0.0113,  0.0099,  0.0196],\n",
       "                      [ 0.0021, -0.0336, -0.0163,  ..., -0.0157,  0.0181, -0.0082],\n",
       "                      [-0.0224, -0.0079,  0.0280,  ...,  0.0146,  0.0288,  0.0019],\n",
       "                      ...,\n",
       "                      [-0.0545,  0.0080, -0.0214,  ...,  0.0098, -0.0019, -0.0144],\n",
       "                      [-0.0415, -0.0182,  0.0443,  ..., -0.0317,  0.0006,  0.0322],\n",
       "                      [ 0.0104,  0.0140,  0.0129,  ..., -0.0164, -0.0255, -0.0122]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.7.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.7.norm2.weight',\n",
       "              tensor([[ 4.4524e-03,  2.6100e-03,  1.8047e-02,  ..., -2.3651e-03,\n",
       "                        1.1843e-02,  6.1416e-03],\n",
       "                      [ 2.4614e-02,  8.3755e-05, -2.4240e-03,  ..., -1.7182e-02,\n",
       "                        1.6482e-03,  2.0140e-02],\n",
       "                      [ 7.8868e-03,  8.6039e-03, -1.0730e-02,  ...,  4.0656e-02,\n",
       "                        3.0219e-02,  1.8923e-02],\n",
       "                      ...,\n",
       "                      [ 2.6472e-02, -9.2652e-03, -2.2737e-02,  ..., -8.1323e-03,\n",
       "                        1.8405e-02, -7.4473e-03],\n",
       "                      [ 3.4539e-02,  4.2467e-03,  1.5182e-02,  ..., -1.5844e-02,\n",
       "                        1.4849e-02,  7.8740e-03],\n",
       "                      [ 2.0063e-02,  1.5528e-02, -4.5828e-02,  ...,  3.4682e-02,\n",
       "                       -1.0359e-02,  2.5135e-03]], device='cuda:0')),\n",
       "             ('blocks.7.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.7.mlp.fc1.weight',\n",
       "              tensor([[ 0.0130,  0.0136,  0.0115,  ..., -0.0214,  0.0093, -0.0240],\n",
       "                      [-0.0088,  0.0253, -0.0092,  ...,  0.0155, -0.0091, -0.0264],\n",
       "                      [ 0.0638, -0.0037,  0.0078,  ...,  0.0029, -0.0035, -0.0164],\n",
       "                      ...,\n",
       "                      [-0.0151,  0.0087,  0.0033,  ..., -0.0003, -0.0005, -0.0184],\n",
       "                      [ 0.0216,  0.0121,  0.0112,  ..., -0.0311, -0.0141,  0.0335],\n",
       "                      [-0.0114,  0.0074, -0.0011,  ...,  0.0253,  0.0115, -0.0053]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.7.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.7.mlp.fc2.weight',\n",
       "              tensor([[-0.0244,  0.0064,  0.0174,  ..., -0.0003,  0.0005, -0.0142],\n",
       "                      [ 0.0189,  0.0249,  0.0048,  ...,  0.0024,  0.0051,  0.0028],\n",
       "                      [ 0.0018, -0.0096, -0.0043,  ...,  0.0029,  0.0170,  0.0031],\n",
       "                      ...,\n",
       "                      [-0.0079, -0.0246,  0.0101,  ...,  0.0066,  0.0092, -0.0010],\n",
       "                      [ 0.0021,  0.0023,  0.0311,  ...,  0.0023,  0.0210, -0.0188],\n",
       "                      [-0.0364, -0.0141,  0.0092,  ...,  0.0012,  0.0036,  0.0301]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.7.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.7.gamma_1.weight',\n",
       "              tensor([[-0.0253, -0.0178,  0.0039,  ...,  0.0002, -0.0249,  0.0024],\n",
       "                      [-0.0091, -0.0357,  0.0542,  ..., -0.0100,  0.0040,  0.0086],\n",
       "                      [-0.0116, -0.0288, -0.0027,  ...,  0.0071, -0.0343,  0.0084],\n",
       "                      ...,\n",
       "                      [ 0.0026,  0.0150, -0.0294,  ..., -0.0041, -0.0011,  0.0206],\n",
       "                      [-0.0060, -0.0418, -0.0637,  ...,  0.0182,  0.0282,  0.0354],\n",
       "                      [ 0.0025,  0.0189,  0.0046,  ...,  0.0136,  0.0130, -0.0080]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.7.gamma_2.weight',\n",
       "              tensor([[-0.0378,  0.0195, -0.0081,  ...,  0.0610, -0.0252, -0.0089],\n",
       "                      [ 0.0144, -0.0211, -0.0214,  ...,  0.0141,  0.0196, -0.0473],\n",
       "                      [ 0.0012,  0.0123, -0.0035,  ...,  0.0062,  0.0098,  0.0049],\n",
       "                      ...,\n",
       "                      [-0.0118,  0.0306, -0.0277,  ..., -0.0112,  0.0194, -0.0016],\n",
       "                      [-0.0128,  0.0100, -0.0401,  ..., -0.0222, -0.0561,  0.0194],\n",
       "                      [ 0.0272, -0.0085, -0.0117,  ...,  0.0076, -0.0068, -0.0331]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.8.norm1.weight',\n",
       "              tensor([[ 0.0052, -0.0162,  0.0196,  ...,  0.0293,  0.0191,  0.0158],\n",
       "                      [ 0.0076, -0.0021, -0.0049,  ...,  0.0157,  0.0073,  0.0460],\n",
       "                      [-0.0307,  0.0275, -0.0247,  ..., -0.0076, -0.0378,  0.0047],\n",
       "                      ...,\n",
       "                      [ 0.0218, -0.0181, -0.0263,  ...,  0.0017, -0.0361, -0.0204],\n",
       "                      [-0.0094,  0.0013, -0.0180,  ...,  0.0159, -0.0090, -0.0019],\n",
       "                      [-0.0226,  0.0129, -0.0306,  ..., -0.0094, -0.0150,  0.0604]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.8.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.8.attn.weight',\n",
       "              tensor([[ 0.0471, -0.0155,  0.0117,  ..., -0.0046, -0.0154,  0.0045],\n",
       "                      [ 0.0245, -0.0255, -0.0395,  ..., -0.0053, -0.0248,  0.0196],\n",
       "                      [-0.0096,  0.0027,  0.0149,  ..., -0.0059,  0.0114,  0.0290],\n",
       "                      ...,\n",
       "                      [-0.0074, -0.0212,  0.0028,  ..., -0.0119, -0.0020, -0.0036],\n",
       "                      [ 0.0109,  0.0361, -0.0260,  ..., -0.0140, -0.0091, -0.0044],\n",
       "                      [ 0.0022,  0.0247,  0.0173,  ...,  0.0103, -0.0029, -0.0034]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.8.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.8.norm2.weight',\n",
       "              tensor([[ 0.0037,  0.0319, -0.0342,  ...,  0.0119,  0.0217,  0.0114],\n",
       "                      [ 0.0320,  0.0113,  0.0041,  ...,  0.0230, -0.0339, -0.0149],\n",
       "                      [-0.0035,  0.0038, -0.0122,  ..., -0.0095,  0.0178,  0.0159],\n",
       "                      ...,\n",
       "                      [ 0.0140, -0.0035,  0.0160,  ...,  0.0014,  0.0289, -0.0375],\n",
       "                      [ 0.0105, -0.0155, -0.0448,  ...,  0.0189,  0.0139,  0.0315],\n",
       "                      [-0.0272, -0.0058, -0.0140,  ..., -0.0241,  0.0092, -0.0281]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.8.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.8.mlp.fc1.weight',\n",
       "              tensor([[-0.0105, -0.0174, -0.0220,  ..., -0.0155, -0.0240,  0.0285],\n",
       "                      [ 0.0504,  0.0244,  0.0240,  ...,  0.0232,  0.0079, -0.0091],\n",
       "                      [-0.0170,  0.0285, -0.0081,  ...,  0.0115,  0.0296,  0.0044],\n",
       "                      ...,\n",
       "                      [ 0.0326, -0.0197, -0.0244,  ...,  0.0091,  0.0331,  0.0060],\n",
       "                      [ 0.0099,  0.0417,  0.0237,  ..., -0.0124, -0.0139,  0.0061],\n",
       "                      [-0.0201, -0.0057, -0.0116,  ...,  0.0240,  0.0104, -0.0080]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.8.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.8.mlp.fc2.weight',\n",
       "              tensor([[-0.0104,  0.0072, -0.0067,  ...,  0.0232, -0.0082,  0.0025],\n",
       "                      [ 0.0172,  0.0022,  0.0093,  ..., -0.0238,  0.0203,  0.0358],\n",
       "                      [-0.0478, -0.0143,  0.0070,  ...,  0.0126, -0.0022,  0.0103],\n",
       "                      ...,\n",
       "                      [-0.0076, -0.0184,  0.0245,  ..., -0.0196, -0.0046, -0.0448],\n",
       "                      [-0.0126, -0.0119, -0.0027,  ...,  0.0233,  0.0573,  0.0141],\n",
       "                      [-0.0250, -0.0036,  0.0328,  ..., -0.0025, -0.0032,  0.0105]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.8.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.8.gamma_1.weight',\n",
       "              tensor([[ 0.0166,  0.0022,  0.0079,  ..., -0.0393, -0.0436,  0.0101],\n",
       "                      [ 0.0366,  0.0096,  0.0204,  ..., -0.0372, -0.0260, -0.0027],\n",
       "                      [ 0.0155,  0.0457, -0.0165,  ..., -0.0265, -0.0194,  0.0043],\n",
       "                      ...,\n",
       "                      [-0.0239,  0.0111, -0.0251,  ...,  0.0063, -0.0088,  0.0017],\n",
       "                      [ 0.0149, -0.0200,  0.0215,  ...,  0.0211,  0.0055,  0.0484],\n",
       "                      [ 0.0127,  0.0073, -0.0200,  ...,  0.0335,  0.0270, -0.0129]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.8.gamma_2.weight',\n",
       "              tensor([[ 0.0408,  0.0022,  0.0050,  ..., -0.0118,  0.0162, -0.0394],\n",
       "                      [-0.0243, -0.0041,  0.0035,  ...,  0.0026,  0.0330, -0.0308],\n",
       "                      [ 0.0220,  0.0103, -0.0045,  ..., -0.0367,  0.0065, -0.0013],\n",
       "                      ...,\n",
       "                      [-0.0151, -0.0401, -0.0050,  ..., -0.0202, -0.0084,  0.0077],\n",
       "                      [ 0.0109,  0.0296, -0.0042,  ...,  0.0208,  0.0608,  0.0028],\n",
       "                      [ 0.0048,  0.0041,  0.0072,  ...,  0.0118,  0.0057, -0.0063]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.9.norm1.weight',\n",
       "              tensor([[-0.0093, -0.0020,  0.0157,  ...,  0.0213, -0.0007, -0.0058],\n",
       "                      [ 0.0261,  0.0293,  0.0106,  ..., -0.0164,  0.0217,  0.0343],\n",
       "                      [-0.0018, -0.0139, -0.0323,  ...,  0.0186, -0.0053, -0.0045],\n",
       "                      ...,\n",
       "                      [ 0.0146,  0.0094,  0.0244,  ...,  0.0160, -0.0074,  0.0011],\n",
       "                      [ 0.0294, -0.0065, -0.0008,  ...,  0.0050,  0.0215,  0.0218],\n",
       "                      [-0.0351, -0.0251,  0.0327,  ...,  0.0074, -0.0246, -0.0044]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.9.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.9.attn.weight',\n",
       "              tensor([[ 0.0038, -0.0071, -0.0322,  ..., -0.0201, -0.0086,  0.0069],\n",
       "                      [-0.0060,  0.0489,  0.0224,  ..., -0.0076, -0.0051, -0.0033],\n",
       "                      [ 0.0045,  0.0191, -0.0063,  ..., -0.0443,  0.0308, -0.0292],\n",
       "                      ...,\n",
       "                      [ 0.0372,  0.0119, -0.0106,  ..., -0.0532,  0.0405, -0.0113],\n",
       "                      [ 0.0231, -0.0101, -0.0096,  ...,  0.0435,  0.0200,  0.0273],\n",
       "                      [-0.0108, -0.0423,  0.0277,  ..., -0.0113,  0.0059,  0.0300]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.9.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.9.norm2.weight',\n",
       "              tensor([[ 0.0064, -0.0391, -0.0033,  ...,  0.0284,  0.0102,  0.0003],\n",
       "                      [ 0.0057,  0.0284, -0.0127,  ...,  0.0281, -0.0055,  0.0040],\n",
       "                      [-0.0123,  0.0399, -0.0012,  ...,  0.0411,  0.0105, -0.0436],\n",
       "                      ...,\n",
       "                      [-0.0277, -0.0014, -0.0123,  ..., -0.0041, -0.0035,  0.0323],\n",
       "                      [-0.0239, -0.0077,  0.0163,  ..., -0.0040,  0.0392, -0.0043],\n",
       "                      [ 0.0099,  0.0017,  0.0025,  ...,  0.0036,  0.0269,  0.0164]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.9.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.9.mlp.fc1.weight',\n",
       "              tensor([[ 0.0245,  0.0020,  0.0236,  ..., -0.0206,  0.0039,  0.0141],\n",
       "                      [-0.0020, -0.0279,  0.0030,  ..., -0.0087, -0.0072, -0.0019],\n",
       "                      [ 0.0131,  0.0052, -0.0090,  ...,  0.0025, -0.0072,  0.0387],\n",
       "                      ...,\n",
       "                      [ 0.0004, -0.0080,  0.0140,  ...,  0.0043,  0.0075, -0.0051],\n",
       "                      [ 0.0143, -0.0194, -0.0061,  ..., -0.0408,  0.0141, -0.0050],\n",
       "                      [ 0.0088,  0.0117,  0.0521,  ...,  0.0085, -0.0082, -0.0522]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.9.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.9.mlp.fc2.weight',\n",
       "              tensor([[ 3.3604e-03, -1.1275e-02, -1.1370e-02,  ..., -3.1524e-04,\n",
       "                       -6.8202e-03,  1.2221e-02],\n",
       "                      [ 2.4401e-02,  1.0729e-02, -2.0327e-02,  ...,  2.5397e-02,\n",
       "                       -2.7747e-03,  5.5842e-03],\n",
       "                      [-1.9360e-02, -1.6317e-02,  1.3095e-02,  ...,  4.5198e-02,\n",
       "                        1.7224e-02, -1.9406e-02],\n",
       "                      ...,\n",
       "                      [ 1.7030e-02, -1.5279e-02, -2.2208e-04,  ..., -1.4918e-02,\n",
       "                        9.0649e-03,  2.7115e-03],\n",
       "                      [-5.7744e-03,  4.7274e-03, -2.9861e-02,  ..., -3.1112e-02,\n",
       "                       -6.3224e-03, -5.8011e-03],\n",
       "                      [ 1.2234e-02, -5.3045e-05, -1.6843e-02,  ..., -1.0568e-02,\n",
       "                       -3.3436e-02, -2.6894e-02]], device='cuda:0')),\n",
       "             ('blocks.9.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.9.gamma_1.weight',\n",
       "              tensor([[-0.0039, -0.0070, -0.0339,  ..., -0.0113,  0.0059, -0.0126],\n",
       "                      [ 0.0316, -0.0020,  0.0110,  ..., -0.0173, -0.0061,  0.0073],\n",
       "                      [ 0.0016,  0.0288,  0.0121,  ...,  0.0227, -0.0444,  0.0107],\n",
       "                      ...,\n",
       "                      [ 0.0036,  0.0099,  0.0136,  ...,  0.0128,  0.0111, -0.0125],\n",
       "                      [-0.0283, -0.0306,  0.0149,  ..., -0.0480,  0.0013,  0.0148],\n",
       "                      [-0.0012, -0.0117,  0.0048,  ...,  0.0035,  0.0006, -0.0136]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.9.gamma_2.weight',\n",
       "              tensor([[ 1.7705e-02,  1.9156e-02, -9.1052e-03,  ...,  3.7769e-03,\n",
       "                        1.2374e-02,  1.2521e-02],\n",
       "                      [-3.2247e-04, -2.5031e-03, -1.3920e-02,  ..., -4.5147e-02,\n",
       "                        2.9412e-03, -5.2291e-04],\n",
       "                      [-3.9076e-02, -1.8911e-02, -1.7741e-02,  ...,  1.9554e-02,\n",
       "                       -5.3237e-04, -3.2274e-02],\n",
       "                      ...,\n",
       "                      [-6.5436e-03,  3.8795e-03, -1.4466e-03,  ..., -3.0955e-02,\n",
       "                       -1.0534e-02,  6.3100e-03],\n",
       "                      [ 2.1884e-02, -2.1932e-02, -3.4695e-02,  ...,  1.3190e-03,\n",
       "                       -1.2101e-02,  2.3881e-02],\n",
       "                      [ 3.0635e-02,  3.6284e-02, -5.5376e-05,  ...,  2.3400e-02,\n",
       "                        3.5820e-03, -3.7047e-02]], device='cuda:0')),\n",
       "             ('blocks.10.norm1.weight',\n",
       "              tensor([[-0.0048, -0.0074,  0.0177,  ..., -0.0061,  0.0266, -0.0144],\n",
       "                      [ 0.0203, -0.0446,  0.0487,  ..., -0.0053, -0.0069, -0.0032],\n",
       "                      [-0.0215,  0.0268, -0.0588,  ...,  0.0150, -0.0262, -0.0232],\n",
       "                      ...,\n",
       "                      [ 0.0109, -0.0111,  0.0094,  ..., -0.0330,  0.0024, -0.0231],\n",
       "                      [-0.0129,  0.0516,  0.0217,  ..., -0.0001,  0.0016,  0.0048],\n",
       "                      [ 0.0152,  0.0003,  0.0374,  ...,  0.0221, -0.0231, -0.0197]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.10.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.10.attn.weight',\n",
       "              tensor([[ 0.0211, -0.0068,  0.0011,  ...,  0.0025, -0.0269, -0.0069],\n",
       "                      [ 0.0111,  0.0078, -0.0209,  ...,  0.0274, -0.0058, -0.0085],\n",
       "                      [-0.0289,  0.0462, -0.0283,  ...,  0.0091, -0.0070,  0.0152],\n",
       "                      ...,\n",
       "                      [-0.0048, -0.0141,  0.0191,  ...,  0.0113,  0.0342, -0.0019],\n",
       "                      [ 0.0031,  0.0052, -0.0064,  ...,  0.0282,  0.0037,  0.0149],\n",
       "                      [ 0.0138, -0.0246, -0.0080,  ...,  0.0054, -0.0061, -0.0030]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.10.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.10.norm2.weight',\n",
       "              tensor([[ 0.0078,  0.0186,  0.0029,  ...,  0.0133, -0.0032, -0.0185],\n",
       "                      [-0.0383, -0.0267, -0.0485,  ..., -0.0085,  0.0257, -0.0094],\n",
       "                      [ 0.0149, -0.0356, -0.0269,  ..., -0.0271,  0.0023, -0.0167],\n",
       "                      ...,\n",
       "                      [ 0.0401, -0.0012,  0.0101,  ...,  0.0100, -0.0106,  0.0289],\n",
       "                      [ 0.0358,  0.0002, -0.0133,  ...,  0.0132,  0.0230, -0.0372],\n",
       "                      [-0.0012, -0.0321,  0.0107,  ..., -0.0066,  0.0174, -0.0007]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.10.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.10.mlp.fc1.weight',\n",
       "              tensor([[-2.0535e-02, -3.4193e-02,  2.1768e-02,  ...,  1.6434e-02,\n",
       "                        1.5251e-05,  8.2334e-03],\n",
       "                      [-2.2877e-02,  4.7802e-02, -8.6865e-03,  ...,  1.7977e-02,\n",
       "                        5.6974e-03, -1.5746e-02],\n",
       "                      [ 1.9356e-02, -1.8907e-02, -1.8898e-02,  ...,  8.6856e-03,\n",
       "                        2.3551e-02,  1.9432e-02],\n",
       "                      ...,\n",
       "                      [-9.0908e-03,  1.4192e-02, -2.6811e-02,  ...,  2.7327e-02,\n",
       "                       -1.3693e-02,  7.3258e-03],\n",
       "                      [-2.2167e-02, -3.9354e-03,  1.0506e-02,  ..., -6.4932e-03,\n",
       "                       -1.9448e-02,  1.7042e-02],\n",
       "                      [-1.4190e-02, -5.2891e-02,  1.5327e-02,  ..., -7.9494e-03,\n",
       "                        5.6795e-03,  7.8073e-04]], device='cuda:0')),\n",
       "             ('blocks.10.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.10.mlp.fc2.weight',\n",
       "              tensor([[-0.0135,  0.0272, -0.0046,  ..., -0.0413, -0.0288, -0.0162],\n",
       "                      [ 0.0197, -0.0102,  0.0023,  ...,  0.0122, -0.0209,  0.0108],\n",
       "                      [ 0.0180, -0.0115,  0.0085,  ...,  0.0343, -0.0082,  0.0115],\n",
       "                      ...,\n",
       "                      [-0.0188,  0.0309,  0.0222,  ..., -0.0128,  0.0162, -0.0057],\n",
       "                      [-0.0088,  0.0494,  0.0100,  ...,  0.0083,  0.0301, -0.0422],\n",
       "                      [-0.0362,  0.0446, -0.0191,  ..., -0.0068, -0.0189, -0.0141]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.10.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.10.gamma_1.weight',\n",
       "              tensor([[ 0.0195, -0.0281, -0.0244,  ...,  0.0288,  0.0201,  0.0272],\n",
       "                      [-0.0051,  0.0052, -0.0159,  ...,  0.0081, -0.0247, -0.0136],\n",
       "                      [-0.0057, -0.0146, -0.0196,  ..., -0.0056,  0.0536, -0.0267],\n",
       "                      ...,\n",
       "                      [ 0.0142, -0.0191, -0.0032,  ...,  0.0072, -0.0044,  0.0429],\n",
       "                      [ 0.0088, -0.0021,  0.0337,  ..., -0.0314, -0.0114,  0.0172],\n",
       "                      [ 0.0188,  0.0110,  0.0096,  ..., -0.0210,  0.0286,  0.0487]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.10.gamma_2.weight',\n",
       "              tensor([[ 4.0798e-02,  2.0804e-02, -1.2911e-02,  ...,  1.1237e-02,\n",
       "                        1.4233e-02, -7.4571e-03],\n",
       "                      [-2.2873e-02,  1.7567e-02, -3.9495e-02,  ...,  2.8666e-03,\n",
       "                        6.0695e-03, -1.4518e-02],\n",
       "                      [ 9.1813e-04,  1.2242e-02, -1.4719e-02,  ...,  1.7139e-02,\n",
       "                       -3.6184e-03,  4.9934e-03],\n",
       "                      ...,\n",
       "                      [-8.9062e-03,  1.2029e-02,  1.3288e-02,  ...,  1.0305e-02,\n",
       "                        2.4076e-02,  1.6749e-02],\n",
       "                      [ 4.1907e-02,  1.1648e-02,  8.4939e-03,  ..., -3.5821e-02,\n",
       "                       -3.2174e-02, -2.9480e-02],\n",
       "                      [-9.1360e-03,  4.9409e-03,  9.6174e-03,  ..., -6.6872e-05,\n",
       "                       -3.2317e-02, -1.2237e-03]], device='cuda:0')),\n",
       "             ('blocks.11.norm1.weight',\n",
       "              tensor([[ 1.5013e-02, -1.0898e-02, -2.7133e-03,  ..., -2.0041e-02,\n",
       "                       -1.0026e-02, -2.3610e-03],\n",
       "                      [-2.0126e-02, -3.0132e-02,  1.1114e-03,  ..., -8.0952e-03,\n",
       "                       -2.8722e-02, -5.0044e-03],\n",
       "                      [ 2.1963e-02, -9.8764e-03,  1.4554e-02,  ...,  1.9954e-02,\n",
       "                       -2.5989e-02,  1.2047e-02],\n",
       "                      ...,\n",
       "                      [ 1.7911e-02, -1.6934e-03, -2.0001e-03,  ..., -1.8725e-02,\n",
       "                       -1.9202e-02, -2.8220e-05],\n",
       "                      [-7.1876e-03, -1.2328e-02, -1.5606e-02,  ..., -4.2148e-02,\n",
       "                       -8.2683e-03,  8.6376e-03],\n",
       "                      [-3.5356e-03, -4.6000e-03,  1.0522e-02,  ...,  8.7408e-03,\n",
       "                        2.0371e-02,  5.1781e-03]], device='cuda:0')),\n",
       "             ('blocks.11.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.11.attn.weight',\n",
       "              tensor([[-0.0038,  0.0016, -0.0315,  ...,  0.0049, -0.0197,  0.0119],\n",
       "                      [ 0.0048,  0.0199, -0.0450,  ...,  0.0058, -0.0235,  0.0531],\n",
       "                      [-0.0168, -0.0199, -0.0114,  ...,  0.0070,  0.0170, -0.0101],\n",
       "                      ...,\n",
       "                      [-0.0310, -0.0196,  0.0140,  ...,  0.0242,  0.0142,  0.0228],\n",
       "                      [-0.0032,  0.0038,  0.0046,  ..., -0.0300,  0.0268,  0.0268],\n",
       "                      [ 0.0041, -0.0170,  0.0124,  ..., -0.0065, -0.0027,  0.0170]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.11.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.11.norm2.weight',\n",
       "              tensor([[-0.0313,  0.0123,  0.0126,  ...,  0.0048,  0.0092,  0.0324],\n",
       "                      [-0.0112,  0.0193, -0.0052,  ..., -0.0111, -0.0152, -0.0160],\n",
       "                      [-0.0288, -0.0518, -0.0356,  ..., -0.0039, -0.0069,  0.0137],\n",
       "                      ...,\n",
       "                      [ 0.0028,  0.0115,  0.0132,  ...,  0.0229,  0.0069, -0.0005],\n",
       "                      [-0.0349, -0.0205,  0.0044,  ...,  0.0256,  0.0136, -0.0042],\n",
       "                      [-0.0280,  0.0281, -0.0274,  ..., -0.0329,  0.0333,  0.0072]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.11.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.11.mlp.fc1.weight',\n",
       "              tensor([[ 1.9893e-02,  3.1940e-02, -1.2105e-02,  ...,  1.4911e-03,\n",
       "                       -2.6105e-02, -1.4479e-02],\n",
       "                      [-1.3486e-02,  4.5431e-03,  3.3553e-02,  ...,  6.0901e-03,\n",
       "                        1.6341e-02, -3.1594e-03],\n",
       "                      [ 1.2199e-02,  3.5738e-03,  9.5931e-03,  ..., -1.0553e-02,\n",
       "                        2.4635e-02,  2.1353e-03],\n",
       "                      ...,\n",
       "                      [-8.0168e-03, -2.1288e-03,  1.6051e-02,  ...,  6.7015e-03,\n",
       "                        2.7953e-02,  2.8974e-02],\n",
       "                      [ 6.4064e-03,  1.6853e-06, -1.0646e-02,  ..., -2.6518e-02,\n",
       "                        3.7032e-02, -5.5839e-03],\n",
       "                      [ 1.1567e-02, -1.9868e-03,  1.0652e-02,  ..., -2.5740e-02,\n",
       "                       -3.2457e-02, -2.8604e-02]], device='cuda:0')),\n",
       "             ('blocks.11.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.11.mlp.fc2.weight',\n",
       "              tensor([[-5.6017e-03,  1.7750e-02, -8.4366e-03,  ..., -2.3024e-02,\n",
       "                        1.3481e-02,  6.5244e-03],\n",
       "                      [-1.3163e-02, -1.1213e-02, -1.6169e-02,  ..., -4.0093e-02,\n",
       "                       -2.1505e-03,  2.3734e-02],\n",
       "                      [-4.1179e-03,  3.1374e-02,  1.1036e-02,  ...,  8.5037e-03,\n",
       "                       -1.1932e-02, -2.5761e-02],\n",
       "                      ...,\n",
       "                      [-2.6458e-02,  2.1691e-02, -1.8878e-02,  ...,  2.7045e-02,\n",
       "                       -3.3403e-03, -1.1943e-02],\n",
       "                      [ 8.0630e-03,  2.2641e-02,  1.3554e-02,  ..., -2.1271e-02,\n",
       "                        4.2235e-02,  3.5917e-06],\n",
       "                      [-1.8518e-02,  4.3777e-02, -1.2537e-02,  ..., -5.0552e-03,\n",
       "                        2.2717e-03,  1.4246e-02]], device='cuda:0')),\n",
       "             ('blocks.11.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.11.gamma_1.weight',\n",
       "              tensor([[-0.0117, -0.0141, -0.0214,  ..., -0.0038, -0.0340,  0.0053],\n",
       "                      [ 0.0045, -0.0327,  0.0061,  ..., -0.0133, -0.0270,  0.0217],\n",
       "                      [ 0.0313, -0.0082, -0.0089,  ..., -0.0106,  0.0155,  0.0097],\n",
       "                      ...,\n",
       "                      [ 0.0047, -0.0048,  0.0083,  ..., -0.0125,  0.0052,  0.0170],\n",
       "                      [-0.0008,  0.0106, -0.0209,  ...,  0.0073,  0.0090,  0.0269],\n",
       "                      [-0.0342, -0.0097, -0.0260,  ..., -0.0199,  0.0252, -0.0196]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.11.gamma_2.weight',\n",
       "              tensor([[ 0.0089, -0.0023, -0.0083,  ...,  0.0100, -0.0188, -0.0113],\n",
       "                      [ 0.0134,  0.0055,  0.0069,  ...,  0.0121,  0.0077,  0.0026],\n",
       "                      [-0.0049,  0.0020, -0.0146,  ...,  0.0033,  0.0001, -0.0101],\n",
       "                      ...,\n",
       "                      [-0.0299, -0.0227,  0.0405,  ...,  0.0007,  0.0083, -0.0062],\n",
       "                      [ 0.0084, -0.0035, -0.0125,  ...,  0.0202,  0.0191, -0.0218],\n",
       "                      [-0.0319, -0.0203, -0.0258,  ..., -0.0142, -0.0212,  0.0079]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.12.norm1.weight',\n",
       "              tensor([[-0.0145, -0.0276,  0.0110,  ..., -0.0418,  0.0346, -0.0143],\n",
       "                      [ 0.0092, -0.0007, -0.0116,  ...,  0.0128, -0.0100, -0.0195],\n",
       "                      [-0.0377, -0.0122, -0.0124,  ...,  0.0245,  0.0018,  0.0295],\n",
       "                      ...,\n",
       "                      [-0.0314, -0.0479,  0.0029,  ..., -0.0548,  0.0148, -0.0153],\n",
       "                      [ 0.0279, -0.0172, -0.0108,  ..., -0.0013,  0.0029,  0.0212],\n",
       "                      [ 0.0141,  0.0007, -0.0119,  ...,  0.0065, -0.0429, -0.0165]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.12.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.12.attn.weight',\n",
       "              tensor([[ 0.0253, -0.0190, -0.0183,  ...,  0.0099,  0.0244,  0.0009],\n",
       "                      [ 0.0250,  0.0211, -0.0023,  ..., -0.0006,  0.0020,  0.0003],\n",
       "                      [ 0.0175,  0.0270,  0.0025,  ..., -0.0358, -0.0377, -0.0103],\n",
       "                      ...,\n",
       "                      [ 0.0182, -0.0391, -0.0195,  ...,  0.0195,  0.0193, -0.0210],\n",
       "                      [-0.0175, -0.0086, -0.0058,  ..., -0.0028, -0.0130,  0.0128],\n",
       "                      [ 0.0461,  0.0087,  0.0136,  ..., -0.0273, -0.0012,  0.0055]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.12.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.12.norm2.weight',\n",
       "              tensor([[ 0.0399,  0.0076,  0.0029,  ...,  0.0116,  0.0128,  0.0134],\n",
       "                      [-0.0466, -0.0195, -0.0151,  ..., -0.0212, -0.0043, -0.0163],\n",
       "                      [-0.0119, -0.0222, -0.0321,  ..., -0.0011,  0.0211,  0.0147],\n",
       "                      ...,\n",
       "                      [ 0.0273,  0.0119, -0.0374,  ...,  0.0104, -0.0190,  0.0020],\n",
       "                      [ 0.0066,  0.0116, -0.0023,  ..., -0.0196,  0.0103, -0.0012],\n",
       "                      [ 0.0218, -0.0168,  0.0020,  ...,  0.0100, -0.0276,  0.0067]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.12.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.12.mlp.fc1.weight',\n",
       "              tensor([[-0.0060,  0.0228,  0.0248,  ...,  0.0136,  0.0122, -0.0150],\n",
       "                      [ 0.0177, -0.0111,  0.0100,  ..., -0.0107, -0.0185, -0.0051],\n",
       "                      [ 0.0018,  0.0186, -0.0103,  ...,  0.0084,  0.0163, -0.0224],\n",
       "                      ...,\n",
       "                      [-0.0053,  0.0094, -0.0327,  ..., -0.0034, -0.0239, -0.0164],\n",
       "                      [-0.0150, -0.0057, -0.0371,  ..., -0.0068, -0.0003, -0.0031],\n",
       "                      [-0.0348, -0.0066,  0.0056,  ..., -0.0094,  0.0103,  0.0003]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.12.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.12.mlp.fc2.weight',\n",
       "              tensor([[-2.0710e-02, -4.2492e-03,  2.5136e-02,  ..., -1.5984e-04,\n",
       "                       -7.1086e-03,  4.0716e-02],\n",
       "                      [-9.5087e-03,  1.9080e-02, -5.2963e-04,  ...,  1.2355e-02,\n",
       "                        6.1986e-05, -1.7996e-02],\n",
       "                      [-2.4744e-02, -7.5317e-03,  6.7246e-03,  ..., -7.3820e-03,\n",
       "                        8.2698e-03, -1.2860e-02],\n",
       "                      ...,\n",
       "                      [ 1.7753e-03,  1.6761e-02,  3.0391e-02,  ..., -2.9391e-02,\n",
       "                       -1.6634e-02, -3.1409e-02],\n",
       "                      [ 3.4891e-02, -6.1807e-03, -1.1358e-02,  ..., -9.2553e-03,\n",
       "                       -2.4940e-02, -3.4900e-02],\n",
       "                      [ 6.0150e-03,  4.0636e-03,  1.1053e-02,  ...,  9.9008e-03,\n",
       "                       -9.6853e-03, -1.5088e-02]], device='cuda:0')),\n",
       "             ('blocks.12.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.12.gamma_1.weight',\n",
       "              tensor([[-2.0397e-02, -2.7555e-02,  2.8154e-03,  ...,  1.3393e-02,\n",
       "                       -1.7147e-02,  1.6405e-02],\n",
       "                      [ 5.4803e-03,  2.1753e-03, -3.9079e-02,  ...,  1.5762e-02,\n",
       "                        2.5301e-02, -1.8187e-02],\n",
       "                      [ 9.4111e-03, -1.4399e-02,  1.2872e-02,  ...,  3.2780e-02,\n",
       "                       -2.1987e-04,  5.0235e-03],\n",
       "                      ...,\n",
       "                      [-6.7090e-03,  2.0920e-02, -1.9494e-02,  ..., -3.1264e-03,\n",
       "                       -5.5558e-02,  1.1651e-02],\n",
       "                      [-1.0195e-02, -2.3910e-02, -7.0837e-05,  ...,  2.1005e-03,\n",
       "                        5.8318e-03,  2.3640e-02],\n",
       "                      [-9.6792e-03,  6.4895e-03,  7.4288e-03,  ...,  2.4072e-02,\n",
       "                        1.2510e-02, -1.1838e-02]], device='cuda:0')),\n",
       "             ('blocks.12.gamma_2.weight',\n",
       "              tensor([[ 0.0337,  0.0020, -0.0137,  ..., -0.0126,  0.0051,  0.0310],\n",
       "                      [ 0.0139,  0.0134,  0.0399,  ..., -0.0203,  0.0150,  0.0078],\n",
       "                      [ 0.0270, -0.0067, -0.0265,  ...,  0.0044, -0.0313,  0.0219],\n",
       "                      ...,\n",
       "                      [-0.0274,  0.0020,  0.0134,  ...,  0.0096, -0.0124,  0.0201],\n",
       "                      [-0.0076, -0.0017,  0.0099,  ...,  0.0165,  0.0465, -0.0022],\n",
       "                      [ 0.0136,  0.0287,  0.0046,  ..., -0.0066,  0.0241,  0.0050]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.13.norm1.weight',\n",
       "              tensor([[ 0.0104,  0.0021, -0.0101,  ..., -0.0179, -0.0023, -0.0514],\n",
       "                      [ 0.0101,  0.0018,  0.0229,  ..., -0.0468, -0.0018, -0.0379],\n",
       "                      [ 0.0064, -0.0097,  0.0225,  ...,  0.0028,  0.0187,  0.0325],\n",
       "                      ...,\n",
       "                      [-0.0048,  0.0144, -0.0332,  ..., -0.0036,  0.0100,  0.0095],\n",
       "                      [-0.0380, -0.0184,  0.0080,  ...,  0.0336,  0.0202,  0.0026],\n",
       "                      [-0.0138,  0.0030,  0.0060,  ..., -0.0108,  0.0034,  0.0106]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.13.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.13.attn.weight',\n",
       "              tensor([[-0.0206, -0.0240, -0.0052,  ..., -0.0067,  0.0357, -0.0075],\n",
       "                      [ 0.0220, -0.0098,  0.0132,  ..., -0.0332, -0.0278, -0.0011],\n",
       "                      [ 0.0258,  0.0287, -0.0350,  ...,  0.0101, -0.0180, -0.0435],\n",
       "                      ...,\n",
       "                      [ 0.0654,  0.0426, -0.0121,  ...,  0.0328, -0.0151, -0.0136],\n",
       "                      [-0.0077,  0.0147, -0.0235,  ...,  0.0053,  0.0047, -0.0368],\n",
       "                      [-0.0280, -0.0447, -0.0194,  ...,  0.0065, -0.0083,  0.0132]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.13.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.13.norm2.weight',\n",
       "              tensor([[-0.0040, -0.0409, -0.0192,  ..., -0.0242,  0.0221, -0.0281],\n",
       "                      [ 0.0098,  0.0200, -0.0336,  ...,  0.0271,  0.0032,  0.0222],\n",
       "                      [-0.0083,  0.0047, -0.0195,  ..., -0.0407, -0.0014,  0.0098],\n",
       "                      ...,\n",
       "                      [-0.0258,  0.0180, -0.0117,  ...,  0.0507,  0.0153,  0.0185],\n",
       "                      [-0.0075, -0.0075,  0.0109,  ...,  0.0153,  0.0076,  0.0161],\n",
       "                      [ 0.0110, -0.0244,  0.0234,  ..., -0.0046,  0.0161, -0.0017]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.13.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.13.mlp.fc1.weight',\n",
       "              tensor([[-2.4026e-02, -1.7104e-02,  2.0984e-02,  ...,  9.4325e-03,\n",
       "                        1.8016e-02, -1.4207e-02],\n",
       "                      [ 1.1638e-02, -9.6458e-04, -5.5520e-03,  ...,  7.6451e-03,\n",
       "                       -4.2458e-03, -1.6099e-02],\n",
       "                      [-2.0567e-02,  3.1831e-02, -2.9057e-02,  ...,  1.5175e-02,\n",
       "                        1.7968e-02, -2.8303e-02],\n",
       "                      ...,\n",
       "                      [ 2.7543e-02, -1.6589e-02, -7.8951e-03,  ..., -5.0918e-05,\n",
       "                       -1.0650e-03,  9.2403e-03],\n",
       "                      [ 2.5043e-02, -5.3954e-05, -4.1360e-02,  ..., -3.8172e-03,\n",
       "                        4.7937e-02,  1.9799e-02],\n",
       "                      [-3.0482e-02,  3.2578e-02,  7.0948e-03,  ...,  3.1347e-02,\n",
       "                        1.3896e-02, -2.5221e-02]], device='cuda:0')),\n",
       "             ('blocks.13.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.13.mlp.fc2.weight',\n",
       "              tensor([[-0.0068,  0.0083, -0.0091,  ...,  0.0062, -0.0343, -0.0078],\n",
       "                      [-0.0090,  0.0069,  0.0151,  ..., -0.0203,  0.0063, -0.0055],\n",
       "                      [-0.0160, -0.0471, -0.0069,  ...,  0.0172,  0.0135, -0.0133],\n",
       "                      ...,\n",
       "                      [-0.0199,  0.0175, -0.0258,  ..., -0.0090,  0.0208,  0.0329],\n",
       "                      [-0.0054,  0.0087, -0.0146,  ...,  0.0180,  0.0155,  0.0354],\n",
       "                      [-0.0011, -0.0088, -0.0152,  ...,  0.0237, -0.0394,  0.0219]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.13.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.13.gamma_1.weight',\n",
       "              tensor([[ 0.0101, -0.0228, -0.0013,  ...,  0.0167,  0.0050, -0.0226],\n",
       "                      [ 0.0142,  0.0048,  0.0124,  ...,  0.0275,  0.0193,  0.0229],\n",
       "                      [-0.0077, -0.0267,  0.0080,  ..., -0.0230,  0.0061, -0.0123],\n",
       "                      ...,\n",
       "                      [-0.0137,  0.0165, -0.0146,  ...,  0.0358,  0.0261, -0.0189],\n",
       "                      [-0.0150,  0.0248,  0.0107,  ...,  0.0021,  0.0107, -0.0159],\n",
       "                      [-0.0131, -0.0233,  0.0063,  ..., -0.0029, -0.0078, -0.0293]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.13.gamma_2.weight',\n",
       "              tensor([[ 0.0077, -0.0077, -0.0109,  ..., -0.0076,  0.0163,  0.0050],\n",
       "                      [ 0.0368, -0.0085,  0.0372,  ..., -0.0040,  0.0187, -0.0015],\n",
       "                      [-0.0103,  0.0422,  0.0242,  ..., -0.0238,  0.0055,  0.0124],\n",
       "                      ...,\n",
       "                      [-0.0020,  0.0138, -0.0103,  ..., -0.0028, -0.0001,  0.0363],\n",
       "                      [-0.0114, -0.0115,  0.0320,  ..., -0.0158,  0.0037, -0.0400],\n",
       "                      [ 0.0331,  0.0176,  0.0380,  ..., -0.0090, -0.0182, -0.0261]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.14.norm1.weight',\n",
       "              tensor([[-0.0320,  0.0152, -0.0025,  ...,  0.0089, -0.0321, -0.0364],\n",
       "                      [-0.0269, -0.0037, -0.0066,  ...,  0.0042,  0.0172, -0.0213],\n",
       "                      [-0.0210, -0.0059, -0.0008,  ..., -0.0326, -0.0136,  0.0241],\n",
       "                      ...,\n",
       "                      [-0.0031,  0.0075,  0.0003,  ..., -0.0065, -0.0398,  0.0264],\n",
       "                      [-0.0261,  0.0114, -0.0178,  ..., -0.0021, -0.0004, -0.0254],\n",
       "                      [-0.0185, -0.0179,  0.0148,  ..., -0.0124, -0.0387, -0.0046]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.14.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.14.attn.weight',\n",
       "              tensor([[-0.0182, -0.0038, -0.0123,  ...,  0.0600, -0.0038, -0.0016],\n",
       "                      [ 0.0040, -0.0166,  0.0103,  ...,  0.0469, -0.0165, -0.0026],\n",
       "                      [ 0.0166, -0.0254, -0.0062,  ..., -0.0308,  0.0068,  0.0186],\n",
       "                      ...,\n",
       "                      [ 0.0095, -0.0004, -0.0083,  ..., -0.0031, -0.0058,  0.0219],\n",
       "                      [-0.0080,  0.0208,  0.0109,  ..., -0.0367, -0.0084,  0.0200],\n",
       "                      [-0.0111,  0.0270, -0.0331,  ...,  0.0014,  0.0052,  0.0064]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.14.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.14.norm2.weight',\n",
       "              tensor([[-0.0068, -0.0054,  0.0008,  ...,  0.0160, -0.0216,  0.0127],\n",
       "                      [-0.0158, -0.0238,  0.0150,  ..., -0.0280, -0.0210, -0.0126],\n",
       "                      [ 0.0011, -0.0189,  0.0283,  ...,  0.0055,  0.0402, -0.0152],\n",
       "                      ...,\n",
       "                      [ 0.0246,  0.0271, -0.0175,  ..., -0.0113,  0.0148, -0.0161],\n",
       "                      [-0.0037, -0.0026, -0.0149,  ..., -0.0523, -0.0082,  0.0047],\n",
       "                      [-0.0284, -0.0202, -0.0404,  ..., -0.0087,  0.0046, -0.0352]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.14.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.14.mlp.fc1.weight',\n",
       "              tensor([[-1.0735e-02, -1.0062e-02,  3.3832e-02,  ..., -4.9848e-05,\n",
       "                        1.5400e-03, -7.6063e-03],\n",
       "                      [-7.7834e-04,  3.1373e-02,  3.9148e-03,  ...,  5.9403e-03,\n",
       "                        8.7046e-03, -1.8490e-02],\n",
       "                      [-5.6301e-03, -1.4846e-02, -1.4563e-03,  ...,  3.2240e-03,\n",
       "                        3.9238e-02, -7.7686e-03],\n",
       "                      ...,\n",
       "                      [-5.9707e-03,  2.3620e-02, -1.5637e-02,  ...,  1.6468e-02,\n",
       "                        4.2788e-03, -1.5685e-02],\n",
       "                      [ 3.2264e-02,  2.0968e-02, -1.1716e-02,  ...,  3.8202e-03,\n",
       "                        3.7943e-03, -2.0244e-03],\n",
       "                      [ 2.9061e-02, -8.7074e-06,  1.6214e-02,  ...,  4.1080e-02,\n",
       "                        1.3600e-02, -8.6872e-03]], device='cuda:0')),\n",
       "             ('blocks.14.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.14.mlp.fc2.weight',\n",
       "              tensor([[ 0.0181, -0.0123, -0.0024,  ...,  0.0036,  0.0067, -0.0104],\n",
       "                      [ 0.0107,  0.0343, -0.0027,  ..., -0.0167,  0.0159,  0.0350],\n",
       "                      [ 0.0348, -0.0221, -0.0325,  ..., -0.0096,  0.0300, -0.0264],\n",
       "                      ...,\n",
       "                      [-0.0060, -0.0475, -0.0200,  ...,  0.0070,  0.0360, -0.0005],\n",
       "                      [-0.0140,  0.0289,  0.0152,  ...,  0.0017,  0.0212,  0.0186],\n",
       "                      [ 0.0005, -0.0160,  0.0163,  ...,  0.0213,  0.0298,  0.0087]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.14.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.14.gamma_1.weight',\n",
       "              tensor([[-0.0074, -0.0076,  0.0008,  ...,  0.0211, -0.0019,  0.0192],\n",
       "                      [ 0.0076,  0.0280,  0.0052,  ...,  0.0307,  0.0063,  0.0214],\n",
       "                      [-0.0214, -0.0199, -0.0151,  ...,  0.0244,  0.0079, -0.0145],\n",
       "                      ...,\n",
       "                      [ 0.0070,  0.0231,  0.0094,  ..., -0.0077,  0.0005,  0.0124],\n",
       "                      [ 0.0248, -0.0138, -0.0299,  ...,  0.0102, -0.0064, -0.0151],\n",
       "                      [ 0.0139, -0.0031,  0.0223,  ...,  0.0169,  0.0068, -0.0116]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.14.gamma_2.weight',\n",
       "              tensor([[ 0.0307, -0.0030,  0.0464,  ..., -0.0255, -0.0057,  0.0115],\n",
       "                      [ 0.0232, -0.0041,  0.0275,  ...,  0.0282, -0.0160, -0.0127],\n",
       "                      [-0.0023,  0.0222,  0.0022,  ...,  0.0075, -0.0234,  0.0267],\n",
       "                      ...,\n",
       "                      [ 0.0282,  0.0061,  0.0085,  ...,  0.0012, -0.0097, -0.0529],\n",
       "                      [-0.0063,  0.0244,  0.0484,  ..., -0.0092, -0.0029, -0.0033],\n",
       "                      [-0.0118, -0.0011, -0.0432,  ...,  0.0242, -0.0231,  0.0207]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.15.norm1.weight',\n",
       "              tensor([[-0.0143, -0.0138, -0.0070,  ...,  0.0312, -0.0323, -0.0135],\n",
       "                      [ 0.0243, -0.0308, -0.0229,  ..., -0.0364,  0.0250, -0.0377],\n",
       "                      [ 0.0371, -0.0371, -0.0250,  ..., -0.0040, -0.0289,  0.0005],\n",
       "                      ...,\n",
       "                      [ 0.0243, -0.0153,  0.0418,  ..., -0.0012,  0.0049, -0.0133],\n",
       "                      [ 0.0246, -0.0333,  0.0353,  ...,  0.0151, -0.0062,  0.0020],\n",
       "                      [ 0.0082, -0.0168, -0.0077,  ...,  0.0161,  0.0109, -0.0014]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.15.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.15.attn.weight',\n",
       "              tensor([[ 0.0242, -0.0173, -0.0032,  ..., -0.0153, -0.0053,  0.0198],\n",
       "                      [ 0.0509,  0.0138,  0.0062,  ..., -0.0157, -0.0105,  0.0283],\n",
       "                      [ 0.0027, -0.0435,  0.0311,  ...,  0.0004, -0.0123,  0.0087],\n",
       "                      ...,\n",
       "                      [ 0.0512, -0.0385,  0.0132,  ...,  0.0035,  0.0183,  0.0064],\n",
       "                      [ 0.0135, -0.0147, -0.0062,  ...,  0.0029,  0.0105, -0.0286],\n",
       "                      [-0.0220,  0.0257, -0.0033,  ..., -0.0078,  0.0331,  0.0178]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.15.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.15.norm2.weight',\n",
       "              tensor([[ 0.0030, -0.0313,  0.0152,  ...,  0.0081,  0.0247,  0.0038],\n",
       "                      [-0.0052, -0.0169,  0.0030,  ..., -0.0300, -0.0096,  0.0185],\n",
       "                      [ 0.0007, -0.0188,  0.0207,  ...,  0.0384, -0.0057,  0.0016],\n",
       "                      ...,\n",
       "                      [ 0.0193, -0.0158,  0.0247,  ..., -0.0012, -0.0226, -0.0165],\n",
       "                      [-0.0086,  0.0358, -0.0054,  ...,  0.0291,  0.0060,  0.0305],\n",
       "                      [ 0.0109,  0.0190,  0.0168,  ...,  0.0023,  0.0154,  0.0190]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.15.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.15.mlp.fc1.weight',\n",
       "              tensor([[ 0.0049, -0.0129, -0.0129,  ...,  0.0385, -0.0253, -0.0153],\n",
       "                      [-0.0085, -0.0122, -0.0161,  ..., -0.0052, -0.0124,  0.0178],\n",
       "                      [-0.0172,  0.0003, -0.0131,  ..., -0.0039, -0.0037, -0.0108],\n",
       "                      ...,\n",
       "                      [-0.0313,  0.0008,  0.0170,  ...,  0.0017, -0.0163, -0.0229],\n",
       "                      [-0.0313, -0.0206, -0.0081,  ...,  0.0022,  0.0133, -0.0216],\n",
       "                      [-0.0110,  0.0178,  0.0083,  ...,  0.0150,  0.0059,  0.0037]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.15.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.15.mlp.fc2.weight',\n",
       "              tensor([[-2.8525e-02,  1.6430e-02, -1.7089e-02,  ..., -1.0409e-02,\n",
       "                        2.4418e-02, -9.6583e-03],\n",
       "                      [-1.3508e-02,  8.3230e-03, -1.0798e-03,  ..., -2.2615e-03,\n",
       "                        3.4765e-02, -1.4460e-03],\n",
       "                      [ 1.5333e-03,  3.5512e-03, -9.5296e-03,  ...,  8.2709e-03,\n",
       "                       -4.0553e-02, -4.1141e-02],\n",
       "                      ...,\n",
       "                      [-3.0654e-03,  1.6775e-02, -1.9748e-02,  ...,  1.5473e-02,\n",
       "                       -1.8951e-02, -2.0897e-04],\n",
       "                      [-1.6842e-02, -1.3437e-03, -8.6024e-03,  ..., -8.3251e-03,\n",
       "                        1.3552e-02, -7.2625e-03],\n",
       "                      [-4.4129e-05, -7.4525e-03,  7.2367e-03,  ...,  3.6785e-02,\n",
       "                        3.7509e-02, -2.6056e-02]], device='cuda:0')),\n",
       "             ('blocks.15.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.15.gamma_1.weight',\n",
       "              tensor([[ 0.0191,  0.0079,  0.0187,  ..., -0.0067, -0.0456,  0.0134],\n",
       "                      [-0.0061,  0.0032,  0.0074,  ...,  0.0015, -0.0002, -0.0372],\n",
       "                      [ 0.0221, -0.0341,  0.0099,  ...,  0.0227, -0.0042,  0.0126],\n",
       "                      ...,\n",
       "                      [ 0.0234,  0.0127, -0.0006,  ...,  0.0092,  0.0160, -0.0120],\n",
       "                      [ 0.0411,  0.0038, -0.0014,  ..., -0.0082,  0.0151, -0.0146],\n",
       "                      [-0.0051, -0.0061,  0.0102,  ...,  0.0161, -0.0361,  0.0178]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.15.gamma_2.weight',\n",
       "              tensor([[-0.0003,  0.0045,  0.0354,  ..., -0.0131, -0.0348, -0.0206],\n",
       "                      [ 0.0059,  0.0019, -0.0096,  ..., -0.0220, -0.0038, -0.0325],\n",
       "                      [ 0.0166,  0.0040,  0.0225,  ...,  0.0130, -0.0013,  0.0381],\n",
       "                      ...,\n",
       "                      [-0.0030,  0.0472, -0.0135,  ..., -0.0185,  0.0092,  0.0496],\n",
       "                      [-0.0006, -0.0114,  0.0008,  ...,  0.0128,  0.0098,  0.0554],\n",
       "                      [ 0.0052, -0.0437, -0.0132,  ..., -0.0144,  0.0194, -0.0282]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.16.norm1.weight',\n",
       "              tensor([[-0.0145,  0.0044, -0.0121,  ..., -0.0118,  0.0012,  0.0010],\n",
       "                      [ 0.0126,  0.0135,  0.0384,  ...,  0.0034,  0.0172,  0.0212],\n",
       "                      [ 0.0103, -0.0132, -0.0111,  ...,  0.0228, -0.0097, -0.0267],\n",
       "                      ...,\n",
       "                      [ 0.0054,  0.0319,  0.0108,  ...,  0.0378,  0.0205,  0.0063],\n",
       "                      [ 0.0179, -0.0100,  0.0348,  ...,  0.0372, -0.0220,  0.0055],\n",
       "                      [-0.0140, -0.0099,  0.0074,  ...,  0.0277, -0.0270,  0.0109]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.16.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.16.attn.weight',\n",
       "              tensor([[ 0.0056, -0.0049,  0.0347,  ...,  0.0032,  0.0127, -0.0241],\n",
       "                      [-0.0060, -0.0160,  0.0458,  ...,  0.0085,  0.0118, -0.0026],\n",
       "                      [-0.0231, -0.0127,  0.0112,  ..., -0.0028,  0.0065,  0.0203],\n",
       "                      ...,\n",
       "                      [-0.0114, -0.0452,  0.0246,  ...,  0.0037,  0.0019, -0.0147],\n",
       "                      [ 0.0134, -0.0069, -0.0294,  ..., -0.0213,  0.0137, -0.0182],\n",
       "                      [ 0.0008,  0.0217,  0.0154,  ...,  0.0063, -0.0242,  0.0113]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.16.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.16.norm2.weight',\n",
       "              tensor([[-0.0205,  0.0096,  0.0285,  ...,  0.0093,  0.0017,  0.0271],\n",
       "                      [ 0.0146, -0.0029,  0.0244,  ...,  0.0347, -0.0247,  0.0049],\n",
       "                      [-0.0249,  0.0171,  0.0316,  ..., -0.0054, -0.0002, -0.0165],\n",
       "                      ...,\n",
       "                      [-0.0039,  0.0011, -0.0193,  ...,  0.0173,  0.0426, -0.0104],\n",
       "                      [-0.0047, -0.0210, -0.0025,  ..., -0.0049, -0.0023,  0.0061],\n",
       "                      [ 0.0151, -0.0379,  0.0128,  ..., -0.0022,  0.0280,  0.0164]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.16.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.16.mlp.fc1.weight',\n",
       "              tensor([[ 0.0068, -0.0207,  0.0090,  ...,  0.0234,  0.0512,  0.0010],\n",
       "                      [ 0.0346, -0.0002, -0.0041,  ...,  0.0007,  0.0089,  0.0064],\n",
       "                      [ 0.0344,  0.0289,  0.0087,  ..., -0.0285,  0.0245, -0.0087],\n",
       "                      ...,\n",
       "                      [ 0.0324, -0.0133,  0.0091,  ...,  0.0271, -0.0147, -0.0072],\n",
       "                      [ 0.0284, -0.0049, -0.0133,  ...,  0.0194,  0.0090, -0.0117],\n",
       "                      [-0.0077, -0.0223,  0.0121,  ...,  0.0078, -0.0087, -0.0365]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.16.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.16.mlp.fc2.weight',\n",
       "              tensor([[ 0.0118,  0.0015,  0.0045,  ...,  0.0527,  0.0446, -0.0331],\n",
       "                      [ 0.0034,  0.0026, -0.0197,  ..., -0.0388, -0.0258,  0.0131],\n",
       "                      [ 0.0277, -0.0164, -0.0080,  ...,  0.0110, -0.0079, -0.0073],\n",
       "                      ...,\n",
       "                      [-0.0122,  0.0191, -0.0097,  ...,  0.0068,  0.0140, -0.0075],\n",
       "                      [-0.0032, -0.0179,  0.0017,  ..., -0.0030, -0.0070, -0.0167],\n",
       "                      [-0.0008, -0.0104,  0.0399,  ..., -0.0104, -0.0677,  0.0166]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.16.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.16.gamma_1.weight',\n",
       "              tensor([[-0.0018, -0.0242,  0.0281,  ..., -0.0199, -0.0094, -0.0079],\n",
       "                      [ 0.0402, -0.0069,  0.0298,  ..., -0.0274, -0.0085,  0.0224],\n",
       "                      [ 0.0018,  0.0124, -0.0116,  ...,  0.0018, -0.0195, -0.0239],\n",
       "                      ...,\n",
       "                      [ 0.0176,  0.0195, -0.0061,  ..., -0.0137,  0.0024,  0.0028],\n",
       "                      [ 0.0082, -0.0067,  0.0341,  ..., -0.0282,  0.0318,  0.0182],\n",
       "                      [-0.0427, -0.0074,  0.0101,  ..., -0.0054,  0.0136,  0.0015]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.16.gamma_2.weight',\n",
       "              tensor([[ 3.3509e-03,  2.5019e-02,  7.6193e-03,  ...,  5.0357e-03,\n",
       "                       -1.7794e-02, -6.7978e-03],\n",
       "                      [ 2.9662e-02,  1.7396e-03, -1.7423e-02,  ...,  3.1215e-03,\n",
       "                       -3.8245e-05, -3.9377e-03],\n",
       "                      [-1.2545e-02, -3.1701e-02,  4.3279e-03,  ..., -1.3721e-02,\n",
       "                        1.9635e-02, -1.0460e-02],\n",
       "                      ...,\n",
       "                      [ 1.6097e-02, -9.3046e-03,  4.4600e-02,  ...,  3.0484e-03,\n",
       "                        2.9080e-03,  4.2126e-04],\n",
       "                      [-2.0881e-02,  4.2613e-03,  3.2960e-02,  ...,  6.5108e-03,\n",
       "                        3.8012e-03, -2.9172e-03],\n",
       "                      [-2.3366e-02,  5.5603e-03, -2.9282e-02,  ..., -8.4203e-03,\n",
       "                       -2.5787e-02,  1.2098e-02]], device='cuda:0')),\n",
       "             ('blocks.17.norm1.weight',\n",
       "              tensor([[ 0.0004,  0.0109, -0.0147,  ..., -0.0084,  0.0143,  0.0261],\n",
       "                      [ 0.0342, -0.0289, -0.0125,  ..., -0.0118,  0.0453, -0.0050],\n",
       "                      [-0.0195, -0.0032, -0.0078,  ..., -0.0289, -0.0228,  0.0475],\n",
       "                      ...,\n",
       "                      [ 0.0021,  0.0283, -0.0484,  ...,  0.0417, -0.0032,  0.0162],\n",
       "                      [-0.0344, -0.0322,  0.0033,  ..., -0.0263, -0.0127, -0.0033],\n",
       "                      [-0.0002, -0.0195, -0.0236,  ...,  0.0234,  0.0192, -0.0050]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.17.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.17.attn.weight',\n",
       "              tensor([[-0.0226, -0.0028, -0.0271,  ...,  0.0142,  0.0272,  0.0261],\n",
       "                      [ 0.0096,  0.0054,  0.0154,  ...,  0.0405,  0.0197,  0.0025],\n",
       "                      [ 0.0597, -0.0006, -0.0154,  ...,  0.0066, -0.0167,  0.0156],\n",
       "                      ...,\n",
       "                      [ 0.0057, -0.0204,  0.0211,  ..., -0.0048, -0.0231,  0.0245],\n",
       "                      [-0.0055,  0.0267, -0.0240,  ...,  0.0141, -0.0170,  0.0383],\n",
       "                      [-0.0003, -0.0304,  0.0266,  ...,  0.0170,  0.0086, -0.0019]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.17.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.17.norm2.weight',\n",
       "              tensor([[ 0.0178, -0.0239, -0.0392,  ...,  0.0019, -0.0041, -0.0082],\n",
       "                      [ 0.0095, -0.0209, -0.0260,  ...,  0.0113, -0.0207, -0.0360],\n",
       "                      [ 0.0070,  0.0254,  0.0223,  ...,  0.0190, -0.0023,  0.0023],\n",
       "                      ...,\n",
       "                      [-0.0123,  0.0346, -0.0088,  ..., -0.0221,  0.0186, -0.0299],\n",
       "                      [-0.0214, -0.0614, -0.0246,  ...,  0.0151,  0.0169,  0.0067],\n",
       "                      [-0.0143,  0.0186, -0.0341,  ..., -0.0106, -0.0007,  0.0302]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.17.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.17.mlp.fc1.weight',\n",
       "              tensor([[ 0.0115,  0.0181, -0.0187,  ..., -0.0098, -0.0115,  0.0045],\n",
       "                      [ 0.0051, -0.0019,  0.0189,  ...,  0.0306, -0.0024,  0.0116],\n",
       "                      [-0.0178, -0.0149, -0.0068,  ..., -0.0182,  0.0340,  0.0100],\n",
       "                      ...,\n",
       "                      [-0.0068, -0.0074, -0.0417,  ...,  0.0179, -0.0457,  0.0255],\n",
       "                      [ 0.0298, -0.0055, -0.0176,  ..., -0.0118, -0.0389,  0.0118],\n",
       "                      [-0.0364, -0.0064,  0.0131,  ...,  0.0159, -0.0412,  0.0196]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.17.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.17.mlp.fc2.weight',\n",
       "              tensor([[ 0.0077, -0.0076, -0.0115,  ...,  0.0222, -0.0174,  0.0004],\n",
       "                      [-0.0016,  0.0224,  0.0262,  ...,  0.0606, -0.0077, -0.0107],\n",
       "                      [-0.0115,  0.0127, -0.0078,  ..., -0.0133, -0.0157,  0.0187],\n",
       "                      ...,\n",
       "                      [ 0.0004,  0.0348,  0.0223,  ..., -0.0011, -0.0110,  0.0102],\n",
       "                      [-0.0154, -0.0004,  0.0123,  ...,  0.0077, -0.0196, -0.0240],\n",
       "                      [-0.0072, -0.0282, -0.0201,  ...,  0.0026,  0.0457, -0.0104]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.17.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.17.gamma_1.weight',\n",
       "              tensor([[-0.0412, -0.0230, -0.0131,  ...,  0.0072,  0.0087,  0.0111],\n",
       "                      [ 0.0111,  0.0329,  0.0249,  ..., -0.0030, -0.0093,  0.0073],\n",
       "                      [ 0.0067,  0.0004, -0.0025,  ..., -0.0089, -0.0151, -0.0232],\n",
       "                      ...,\n",
       "                      [ 0.0241,  0.0176, -0.0223,  ..., -0.0422,  0.0291, -0.0214],\n",
       "                      [-0.0499,  0.0220, -0.0178,  ..., -0.0278,  0.0043, -0.0080],\n",
       "                      [-0.0648, -0.0069,  0.0198,  ...,  0.0110,  0.0215, -0.0350]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.17.gamma_2.weight',\n",
       "              tensor([[ 0.0189,  0.0227,  0.0015,  ...,  0.0090, -0.0263, -0.0120],\n",
       "                      [ 0.0186,  0.0019, -0.0042,  ...,  0.0132, -0.0004,  0.0301],\n",
       "                      [-0.0131, -0.0019, -0.0136,  ...,  0.0020, -0.0018, -0.0037],\n",
       "                      ...,\n",
       "                      [-0.0029, -0.0229, -0.0204,  ..., -0.0239,  0.0032, -0.0088],\n",
       "                      [-0.0106,  0.0164,  0.0296,  ...,  0.0104,  0.0357,  0.0258],\n",
       "                      [ 0.0059, -0.0174, -0.0306,  ..., -0.0065, -0.0178, -0.0150]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.18.norm1.weight',\n",
       "              tensor([[ 0.0026,  0.0102,  0.0008,  ...,  0.0072, -0.0097, -0.0072],\n",
       "                      [ 0.0129,  0.0198, -0.0142,  ...,  0.0053,  0.0246,  0.0188],\n",
       "                      [ 0.0341, -0.0378,  0.0216,  ..., -0.0119, -0.0033, -0.0213],\n",
       "                      ...,\n",
       "                      [-0.0166,  0.0028,  0.0049,  ..., -0.0064,  0.0526, -0.0327],\n",
       "                      [-0.0265, -0.0002, -0.0085,  ..., -0.0410, -0.0093, -0.0232],\n",
       "                      [ 0.0210, -0.0362,  0.0357,  ..., -0.0306, -0.0031, -0.0172]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.18.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.18.attn.weight',\n",
       "              tensor([[-8.7098e-03, -2.1575e-03,  1.8538e-02,  ...,  1.6066e-02,\n",
       "                        1.9878e-02,  4.5695e-02],\n",
       "                      [ 1.4078e-02,  8.8485e-03,  6.3085e-03,  ...,  5.0838e-03,\n",
       "                        3.9510e-02, -2.1427e-02],\n",
       "                      [-3.6341e-02, -1.5183e-02,  7.2753e-03,  ..., -9.7702e-03,\n",
       "                        1.6593e-02,  2.3559e-02],\n",
       "                      ...,\n",
       "                      [ 1.3172e-02,  1.2051e-02,  1.8197e-02,  ..., -2.6777e-03,\n",
       "                       -7.9160e-03, -4.0579e-02],\n",
       "                      [-6.6253e-04, -1.5823e-02, -4.3366e-02,  ..., -1.7197e-02,\n",
       "                        1.1153e-02, -1.4502e-02],\n",
       "                      [ 1.8220e-03,  1.1905e-02,  2.1992e-02,  ..., -4.9182e-03,\n",
       "                        9.1867e-03, -1.6913e-05]], device='cuda:0')),\n",
       "             ('blocks.18.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.18.norm2.weight',\n",
       "              tensor([[-0.0098,  0.0061,  0.0053,  ...,  0.0056,  0.0133,  0.0098],\n",
       "                      [ 0.0031, -0.0292,  0.0159,  ..., -0.0087,  0.0286, -0.0206],\n",
       "                      [-0.0038, -0.0099, -0.0040,  ...,  0.0043, -0.0217, -0.0117],\n",
       "                      ...,\n",
       "                      [ 0.0339,  0.0182, -0.0187,  ...,  0.0258, -0.0027, -0.0081],\n",
       "                      [-0.0046, -0.0029, -0.0041,  ...,  0.0299, -0.0019,  0.0057],\n",
       "                      [-0.0063,  0.0189, -0.0181,  ..., -0.0037, -0.0138,  0.0109]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.18.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.18.mlp.fc1.weight',\n",
       "              tensor([[ 0.0095, -0.0119,  0.0185,  ..., -0.0122, -0.0129, -0.0166],\n",
       "                      [-0.0012, -0.0274,  0.0243,  ..., -0.0004,  0.0421, -0.0116],\n",
       "                      [ 0.0306,  0.0205,  0.0141,  ...,  0.0122, -0.0005,  0.0100],\n",
       "                      ...,\n",
       "                      [-0.0252, -0.0161, -0.0045,  ...,  0.0121, -0.0008,  0.0009],\n",
       "                      [-0.0236, -0.0117,  0.0233,  ..., -0.0120, -0.0073, -0.0004],\n",
       "                      [-0.0055, -0.0233, -0.0009,  ...,  0.0006,  0.0269, -0.0154]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.18.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.18.mlp.fc2.weight',\n",
       "              tensor([[-0.0205,  0.0052,  0.0110,  ..., -0.0220, -0.0152,  0.0154],\n",
       "                      [ 0.0209, -0.0086,  0.0126,  ..., -0.0059, -0.0033,  0.0345],\n",
       "                      [ 0.0368, -0.0038, -0.0033,  ...,  0.0383, -0.0062, -0.0286],\n",
       "                      ...,\n",
       "                      [ 0.0003, -0.0288,  0.0220,  ...,  0.0703,  0.0014, -0.0101],\n",
       "                      [-0.0037, -0.0429, -0.0048,  ...,  0.0007,  0.0159, -0.0064],\n",
       "                      [-0.0048,  0.0004,  0.0079,  ..., -0.0076,  0.0015, -0.0304]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.18.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.18.gamma_1.weight',\n",
       "              tensor([[ 0.0136, -0.0003,  0.0059,  ...,  0.0280, -0.0407, -0.0198],\n",
       "                      [-0.0363, -0.0065, -0.0007,  ...,  0.0312,  0.0124,  0.0314],\n",
       "                      [-0.0550,  0.0285,  0.0046,  ...,  0.0446,  0.0078,  0.0041],\n",
       "                      ...,\n",
       "                      [ 0.0192, -0.0131, -0.0116,  ..., -0.0091, -0.0238,  0.0069],\n",
       "                      [ 0.0329, -0.0076,  0.0033,  ..., -0.0167, -0.0163,  0.0047],\n",
       "                      [-0.0017, -0.0006, -0.0161,  ...,  0.0342, -0.0238,  0.0288]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.18.gamma_2.weight',\n",
       "              tensor([[-0.0185, -0.0043,  0.0181,  ...,  0.0010, -0.0005,  0.0009],\n",
       "                      [ 0.0177,  0.0181,  0.0084,  ...,  0.0293, -0.0181,  0.0053],\n",
       "                      [-0.0082,  0.0150, -0.0125,  ...,  0.0138, -0.0087,  0.0316],\n",
       "                      ...,\n",
       "                      [ 0.0155, -0.0498,  0.0106,  ..., -0.0212, -0.0122, -0.0455],\n",
       "                      [ 0.0359, -0.0309, -0.0111,  ...,  0.0176, -0.0058, -0.0193],\n",
       "                      [ 0.0186,  0.0302,  0.0106,  ..., -0.0110,  0.0313, -0.0099]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.19.norm1.weight',\n",
       "              tensor([[ 0.0157, -0.0076,  0.0199,  ...,  0.0164,  0.0294,  0.0151],\n",
       "                      [ 0.0095, -0.0179, -0.0120,  ...,  0.0118,  0.0213,  0.0139],\n",
       "                      [-0.0390,  0.0036,  0.0170,  ..., -0.0331, -0.0274, -0.0293],\n",
       "                      ...,\n",
       "                      [-0.0087,  0.0342, -0.0117,  ..., -0.0498,  0.0112,  0.0010],\n",
       "                      [ 0.0028, -0.0096,  0.0097,  ...,  0.0019,  0.0175,  0.0004],\n",
       "                      [ 0.0285, -0.0602, -0.0013,  ..., -0.0057,  0.0030,  0.0145]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.19.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.19.attn.weight',\n",
       "              tensor([[ 0.0281,  0.0086,  0.0009,  ..., -0.0232,  0.0042, -0.0127],\n",
       "                      [-0.0262, -0.0126, -0.0132,  ..., -0.0105,  0.0091, -0.0181],\n",
       "                      [ 0.0030,  0.0026, -0.0236,  ...,  0.0004, -0.0078, -0.0182],\n",
       "                      ...,\n",
       "                      [ 0.0080, -0.0187,  0.0131,  ..., -0.0307, -0.0071, -0.0322],\n",
       "                      [ 0.0162, -0.0184, -0.0358,  ..., -0.0066, -0.0003,  0.0008],\n",
       "                      [ 0.0137,  0.0391, -0.0194,  ...,  0.0246, -0.0241,  0.0186]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.19.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.19.norm2.weight',\n",
       "              tensor([[ 0.0216,  0.0010, -0.0451,  ..., -0.0162, -0.0211,  0.0160],\n",
       "                      [-0.0273, -0.0266, -0.0088,  ..., -0.0285,  0.0074,  0.0077],\n",
       "                      [ 0.0257,  0.0148,  0.0120,  ..., -0.0006, -0.0025,  0.0313],\n",
       "                      ...,\n",
       "                      [ 0.0011, -0.0104, -0.0064,  ..., -0.0184, -0.0202, -0.0198],\n",
       "                      [ 0.0213, -0.0095,  0.0131,  ..., -0.0167,  0.0032,  0.0004],\n",
       "                      [ 0.0248,  0.0014, -0.0118,  ..., -0.0122,  0.0023, -0.0163]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.19.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.19.mlp.fc1.weight',\n",
       "              tensor([[ 0.0321, -0.0130, -0.0256,  ...,  0.0361, -0.0130, -0.0287],\n",
       "                      [ 0.0041, -0.0021,  0.0311,  ..., -0.0298, -0.0004,  0.0198],\n",
       "                      [-0.0016, -0.0088, -0.0228,  ..., -0.0290,  0.0267,  0.0283],\n",
       "                      ...,\n",
       "                      [ 0.0399,  0.0029, -0.0197,  ..., -0.0104,  0.0254, -0.0204],\n",
       "                      [-0.0074, -0.0218,  0.0416,  ...,  0.0048,  0.0121, -0.0155],\n",
       "                      [-0.0229,  0.0214, -0.0076,  ..., -0.0078, -0.0018, -0.0138]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.19.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.19.mlp.fc2.weight',\n",
       "              tensor([[ 2.7363e-02, -2.6793e-02,  1.0358e-02,  ...,  1.1200e-02,\n",
       "                        1.2443e-03, -5.9054e-03],\n",
       "                      [ 4.2254e-02,  2.0997e-03,  6.4754e-03,  ..., -2.7202e-02,\n",
       "                        6.7743e-03, -4.4287e-03],\n",
       "                      [ 5.1842e-02,  3.8853e-02, -2.2201e-02,  ...,  3.3051e-02,\n",
       "                       -1.0100e-02,  3.1200e-02],\n",
       "                      ...,\n",
       "                      [-1.5748e-02, -2.2577e-02, -2.0333e-02,  ..., -2.1583e-03,\n",
       "                       -1.2789e-02, -2.4970e-02],\n",
       "                      [ 1.4543e-02,  1.9135e-02,  1.7167e-02,  ...,  3.9370e-02,\n",
       "                       -7.2501e-03, -2.6364e-02],\n",
       "                      [-1.5902e-02,  9.7603e-03, -8.0040e-05,  ..., -4.4618e-03,\n",
       "                        1.7552e-02,  5.2301e-03]], device='cuda:0')),\n",
       "             ('blocks.19.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.19.gamma_1.weight',\n",
       "              tensor([[-0.0141,  0.0125,  0.0338,  ..., -0.0027,  0.0307,  0.0205],\n",
       "                      [ 0.0084, -0.0148, -0.0122,  ..., -0.0185,  0.0234,  0.0405],\n",
       "                      [ 0.0003, -0.0438, -0.0121,  ..., -0.0076,  0.0122, -0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0288,  0.0248,  0.0251,  ...,  0.0190,  0.0266,  0.0003],\n",
       "                      [-0.0228,  0.0076,  0.0286,  ...,  0.0088, -0.0346,  0.0062],\n",
       "                      [ 0.0282,  0.0291, -0.0153,  ..., -0.0182,  0.0100, -0.0047]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.19.gamma_2.weight',\n",
       "              tensor([[-0.0048,  0.0309,  0.0357,  ..., -0.0070,  0.0150,  0.0023],\n",
       "                      [-0.0218,  0.0010, -0.0340,  ..., -0.0466,  0.0270, -0.0172],\n",
       "                      [ 0.0080, -0.0097,  0.0110,  ...,  0.0182, -0.0150, -0.0104],\n",
       "                      ...,\n",
       "                      [ 0.0007,  0.0127,  0.0172,  ...,  0.0081, -0.0040, -0.0030],\n",
       "                      [ 0.0289,  0.0468,  0.0133,  ..., -0.0349,  0.0169,  0.0098],\n",
       "                      [ 0.0273,  0.0099, -0.0198,  ..., -0.0018,  0.0166, -0.0289]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.20.norm1.weight',\n",
       "              tensor([[ 0.0132,  0.0246, -0.0134,  ..., -0.0048, -0.0242,  0.0060],\n",
       "                      [ 0.0046,  0.0165,  0.0169,  ...,  0.0005,  0.0281,  0.0482],\n",
       "                      [ 0.0204,  0.0015, -0.0159,  ..., -0.0029,  0.0032, -0.0412],\n",
       "                      ...,\n",
       "                      [-0.0037,  0.0187,  0.0143,  ..., -0.0084,  0.0211,  0.0063],\n",
       "                      [ 0.0120, -0.0234, -0.0116,  ...,  0.0089, -0.0011, -0.0041],\n",
       "                      [ 0.0104,  0.0084, -0.0138,  ..., -0.0020,  0.0005, -0.0152]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.20.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.20.attn.weight',\n",
       "              tensor([[ 0.0047,  0.0155,  0.0109,  ..., -0.0196,  0.0084,  0.0036],\n",
       "                      [ 0.0315,  0.0179, -0.0136,  ...,  0.0283, -0.0305, -0.0006],\n",
       "                      [-0.0216,  0.0025,  0.0020,  ...,  0.0096,  0.0065, -0.0308],\n",
       "                      ...,\n",
       "                      [-0.0015,  0.0245,  0.0421,  ...,  0.0098, -0.0079, -0.0259],\n",
       "                      [-0.0319, -0.0106,  0.0065,  ..., -0.0112, -0.0085,  0.0075],\n",
       "                      [-0.0342, -0.0021,  0.0119,  ...,  0.0095, -0.0242, -0.0155]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.20.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.20.norm2.weight',\n",
       "              tensor([[ 0.0047, -0.0071,  0.0277,  ...,  0.0116,  0.0035, -0.0020],\n",
       "                      [-0.0162,  0.0274,  0.0108,  ...,  0.0312,  0.0170,  0.0287],\n",
       "                      [-0.0109, -0.0496, -0.0120,  ..., -0.0226, -0.0301,  0.0376],\n",
       "                      ...,\n",
       "                      [-0.0115, -0.0031,  0.0246,  ...,  0.0066, -0.0048, -0.0245],\n",
       "                      [-0.0023, -0.0137, -0.0194,  ...,  0.0242,  0.0058,  0.0303],\n",
       "                      [ 0.0040, -0.0024,  0.0114,  ...,  0.0125,  0.0214,  0.0114]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.20.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.20.mlp.fc1.weight',\n",
       "              tensor([[ 0.0030,  0.0249, -0.0108,  ...,  0.0039, -0.0050,  0.0307],\n",
       "                      [-0.0116,  0.0026,  0.0007,  ...,  0.0159,  0.0015, -0.0068],\n",
       "                      [-0.0082, -0.0151, -0.0251,  ..., -0.0184,  0.0163,  0.0006],\n",
       "                      ...,\n",
       "                      [-0.0227,  0.0290,  0.0059,  ..., -0.0020,  0.0241,  0.0444],\n",
       "                      [ 0.0299, -0.0249,  0.0094,  ..., -0.0034, -0.0328,  0.0203],\n",
       "                      [ 0.0034,  0.0068,  0.0421,  ...,  0.0200,  0.0059,  0.0129]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.20.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.20.mlp.fc2.weight',\n",
       "              tensor([[-0.0212, -0.0159, -0.0012,  ...,  0.0218,  0.0031, -0.0050],\n",
       "                      [ 0.0041, -0.0235,  0.0292,  ...,  0.0176, -0.0376, -0.0170],\n",
       "                      [ 0.0217, -0.0021,  0.0168,  ...,  0.0106,  0.0398, -0.0120],\n",
       "                      ...,\n",
       "                      [ 0.0119,  0.0428, -0.0059,  ...,  0.0103, -0.0081, -0.0015],\n",
       "                      [-0.0270,  0.0041, -0.0209,  ...,  0.0026,  0.0222,  0.0245],\n",
       "                      [-0.0017,  0.0118, -0.0201,  ...,  0.0447,  0.0562, -0.0140]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.20.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.20.gamma_1.weight',\n",
       "              tensor([[ 0.0110, -0.0022,  0.0105,  ..., -0.0039,  0.0369,  0.0435],\n",
       "                      [-0.0061, -0.0084,  0.0067,  ...,  0.0076, -0.0407,  0.0084],\n",
       "                      [-0.0042,  0.0113, -0.0037,  ...,  0.0260,  0.0275, -0.0276],\n",
       "                      ...,\n",
       "                      [ 0.0038, -0.0147, -0.0033,  ...,  0.0045,  0.0465, -0.0168],\n",
       "                      [ 0.0198, -0.0595, -0.0199,  ..., -0.0155, -0.0152, -0.0322],\n",
       "                      [ 0.0035,  0.0126, -0.0023,  ...,  0.0099, -0.0498, -0.0084]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.20.gamma_2.weight',\n",
       "              tensor([[-0.0405, -0.0074, -0.0411,  ...,  0.0195, -0.0216,  0.0083],\n",
       "                      [ 0.0111, -0.0024, -0.0051,  ..., -0.0007, -0.0166, -0.0210],\n",
       "                      [ 0.0023,  0.0039,  0.0178,  ...,  0.0060,  0.0269,  0.0092],\n",
       "                      ...,\n",
       "                      [ 0.0251, -0.0134, -0.0050,  ..., -0.0368, -0.0222,  0.0156],\n",
       "                      [ 0.0049,  0.0226, -0.0008,  ...,  0.0083, -0.0267, -0.0024],\n",
       "                      [ 0.0009,  0.0051,  0.0079,  ...,  0.0011,  0.0017, -0.0050]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.21.norm1.weight',\n",
       "              tensor([[ 0.0021,  0.0022, -0.0018,  ...,  0.0175,  0.0285, -0.0068],\n",
       "                      [-0.0205, -0.0044, -0.0103,  ...,  0.0330,  0.0010,  0.0126],\n",
       "                      [ 0.0409, -0.0052, -0.0087,  ...,  0.0233, -0.0379,  0.0068],\n",
       "                      ...,\n",
       "                      [ 0.0008, -0.0300, -0.0125,  ..., -0.0061,  0.0033,  0.0021],\n",
       "                      [ 0.0149,  0.0071,  0.0125,  ...,  0.0275, -0.0156, -0.0165],\n",
       "                      [ 0.0011, -0.0037,  0.0049,  ...,  0.0062, -0.0369, -0.0220]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.21.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.21.attn.weight',\n",
       "              tensor([[ 0.0218,  0.0004,  0.0088,  ..., -0.0056, -0.0247,  0.0105],\n",
       "                      [ 0.0209, -0.0157,  0.0033,  ..., -0.0045,  0.0179, -0.0043],\n",
       "                      [ 0.0165,  0.0117, -0.0179,  ...,  0.0022, -0.0069,  0.0230],\n",
       "                      ...,\n",
       "                      [-0.0351,  0.0159,  0.0129,  ...,  0.0424, -0.0230, -0.0313],\n",
       "                      [-0.0117, -0.0078, -0.0307,  ..., -0.0054,  0.0081,  0.0108],\n",
       "                      [ 0.0214,  0.0124, -0.0459,  ...,  0.0006, -0.0167, -0.0125]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.21.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.21.norm2.weight',\n",
       "              tensor([[ 0.0014,  0.0063,  0.0063,  ...,  0.0331,  0.0172,  0.0022],\n",
       "                      [-0.0106,  0.0010, -0.0368,  ..., -0.0048,  0.0129,  0.0021],\n",
       "                      [-0.0262,  0.0206, -0.0115,  ..., -0.0129,  0.0200,  0.0189],\n",
       "                      ...,\n",
       "                      [ 0.0342,  0.0024,  0.0030,  ...,  0.0192, -0.0102, -0.0053],\n",
       "                      [-0.0083,  0.0032,  0.0539,  ..., -0.0277,  0.0010, -0.0057],\n",
       "                      [-0.0209, -0.0030, -0.0265,  ..., -0.0019,  0.0077, -0.0245]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.21.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.21.mlp.fc1.weight',\n",
       "              tensor([[ 0.0111,  0.0166, -0.0159,  ..., -0.0112,  0.0127,  0.0063],\n",
       "                      [ 0.0279, -0.0093, -0.0227,  ...,  0.0377,  0.0001, -0.0287],\n",
       "                      [-0.0074, -0.0155, -0.0133,  ...,  0.0145,  0.0232, -0.0111],\n",
       "                      ...,\n",
       "                      [-0.0093,  0.0049, -0.0031,  ...,  0.0324,  0.0114, -0.0035],\n",
       "                      [-0.0076, -0.0156,  0.0036,  ...,  0.0114,  0.0106, -0.0023],\n",
       "                      [-0.0197, -0.0053, -0.0081,  ..., -0.0214,  0.0181,  0.0040]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.21.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.21.mlp.fc2.weight',\n",
       "              tensor([[ 3.9061e-02,  7.1141e-03,  1.8663e-02,  ..., -2.3424e-02,\n",
       "                       -2.4325e-02,  4.0389e-03],\n",
       "                      [ 6.1467e-03,  1.7718e-02, -6.6599e-03,  ...,  1.3484e-03,\n",
       "                       -2.8590e-03, -1.7027e-02],\n",
       "                      [-1.2951e-02, -5.3938e-04,  2.0958e-02,  ..., -7.6318e-03,\n",
       "                        2.0134e-02, -2.0665e-02],\n",
       "                      ...,\n",
       "                      [-3.1341e-02,  1.7231e-02,  2.8823e-02,  ..., -2.4024e-02,\n",
       "                       -2.2897e-02, -2.4321e-02],\n",
       "                      [-8.1352e-05, -6.6118e-03, -2.6928e-02,  ..., -1.0380e-02,\n",
       "                        3.1528e-03,  1.8830e-02],\n",
       "                      [ 1.0911e-02, -3.7762e-02, -4.8622e-03,  ..., -4.6583e-03,\n",
       "                       -4.1543e-02,  9.0825e-03]], device='cuda:0')),\n",
       "             ('blocks.21.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.21.gamma_1.weight',\n",
       "              tensor([[-0.0339, -0.0002, -0.0313,  ...,  0.0144,  0.0063, -0.0156],\n",
       "                      [-0.0024, -0.0216,  0.0325,  ..., -0.0387,  0.0291,  0.0120],\n",
       "                      [-0.0106,  0.0128, -0.0133,  ..., -0.0236,  0.0057,  0.0220],\n",
       "                      ...,\n",
       "                      [-0.0054, -0.0112, -0.0130,  ..., -0.0281,  0.0028, -0.0242],\n",
       "                      [-0.0073, -0.0099,  0.0016,  ..., -0.0228,  0.0172, -0.0167],\n",
       "                      [ 0.0009, -0.0237, -0.0107,  ...,  0.0266,  0.0363, -0.0203]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.21.gamma_2.weight',\n",
       "              tensor([[ 0.0096, -0.0229, -0.0050,  ..., -0.0057,  0.0107, -0.0169],\n",
       "                      [ 0.0355,  0.0419, -0.0051,  ...,  0.0189, -0.0123, -0.0231],\n",
       "                      [ 0.0327, -0.0004,  0.0140,  ..., -0.0218, -0.0011, -0.0027],\n",
       "                      ...,\n",
       "                      [ 0.0083,  0.0251,  0.0032,  ...,  0.0126, -0.0247,  0.0199],\n",
       "                      [-0.0315, -0.0008, -0.0037,  ...,  0.0016, -0.0145,  0.0231],\n",
       "                      [-0.0054, -0.0298,  0.0304,  ..., -0.0244,  0.0009, -0.0082]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.22.norm1.weight',\n",
       "              tensor([[-2.0537e-02, -8.0290e-03, -2.5513e-02,  ..., -3.7124e-02,\n",
       "                       -1.3966e-02, -7.4499e-03],\n",
       "                      [ 3.7435e-03, -1.1742e-02, -2.4619e-03,  ..., -5.5684e-03,\n",
       "                       -2.4576e-02,  2.6057e-02],\n",
       "                      [ 9.8673e-03,  1.5718e-02, -1.5868e-02,  ..., -9.6477e-03,\n",
       "                       -6.8282e-03,  3.2845e-02],\n",
       "                      ...,\n",
       "                      [-2.4502e-02, -4.4462e-02, -4.2079e-02,  ..., -1.2208e-02,\n",
       "                       -1.7252e-02, -1.6400e-02],\n",
       "                      [ 1.9653e-02, -1.1904e-02, -9.6758e-03,  ...,  4.8075e-03,\n",
       "                       -5.7175e-05,  1.6417e-02],\n",
       "                      [-2.4796e-02, -3.6997e-02,  2.9930e-03,  ..., -5.1772e-02,\n",
       "                       -7.9113e-03,  1.9899e-02]], device='cuda:0')),\n",
       "             ('blocks.22.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.22.attn.weight',\n",
       "              tensor([[ 0.0269,  0.0506,  0.0288,  ..., -0.0169, -0.0048,  0.0200],\n",
       "                      [-0.0094,  0.0099,  0.0053,  ...,  0.0197,  0.0148,  0.0338],\n",
       "                      [-0.0082, -0.0211, -0.0040,  ..., -0.0138,  0.0189, -0.0455],\n",
       "                      ...,\n",
       "                      [-0.0031, -0.0176, -0.0370,  ..., -0.0157, -0.0066, -0.0166],\n",
       "                      [ 0.0275,  0.0010,  0.0400,  ..., -0.0181, -0.0295, -0.0394],\n",
       "                      [-0.0051, -0.0338, -0.0114,  ..., -0.0343,  0.0106,  0.0050]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.22.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.22.norm2.weight',\n",
       "              tensor([[ 0.0021, -0.0111,  0.0257,  ..., -0.0474, -0.0042, -0.0140],\n",
       "                      [-0.0245, -0.0004, -0.0439,  ...,  0.0074, -0.0422, -0.0171],\n",
       "                      [-0.0028, -0.0073,  0.0006,  ..., -0.0113,  0.0316,  0.0252],\n",
       "                      ...,\n",
       "                      [ 0.0049, -0.0297, -0.0100,  ...,  0.0035,  0.0091,  0.0032],\n",
       "                      [ 0.0156, -0.0274,  0.0194,  ..., -0.0120, -0.0046,  0.0265],\n",
       "                      [ 0.0329,  0.0335,  0.0131,  ...,  0.0009, -0.0047,  0.0236]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.22.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.22.mlp.fc1.weight',\n",
       "              tensor([[-0.0031,  0.0246, -0.0157,  ..., -0.0019,  0.0029,  0.0273],\n",
       "                      [ 0.0119, -0.0120, -0.0330,  ..., -0.0156,  0.0117, -0.0109],\n",
       "                      [ 0.0005, -0.0078,  0.0194,  ...,  0.0359, -0.0184,  0.0076],\n",
       "                      ...,\n",
       "                      [ 0.0152, -0.0245,  0.0183,  ...,  0.0073, -0.0110, -0.0094],\n",
       "                      [ 0.0119,  0.0143, -0.0066,  ..., -0.0267, -0.0155, -0.0374],\n",
       "                      [ 0.0116, -0.0031, -0.0229,  ..., -0.0175,  0.0345,  0.0231]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.22.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.22.mlp.fc2.weight',\n",
       "              tensor([[ 0.0199, -0.0014,  0.0406,  ...,  0.0042,  0.0039,  0.0001],\n",
       "                      [ 0.0120, -0.0032, -0.0057,  ...,  0.0115, -0.0097, -0.0007],\n",
       "                      [ 0.0015,  0.0405, -0.0127,  ...,  0.0007,  0.0029, -0.0210],\n",
       "                      ...,\n",
       "                      [ 0.0022,  0.0210, -0.0113,  ...,  0.0173, -0.0162, -0.0326],\n",
       "                      [ 0.0207, -0.0070,  0.0221,  ..., -0.0162,  0.0285, -0.0509],\n",
       "                      [ 0.0041,  0.0177, -0.0142,  ..., -0.0072,  0.0094, -0.0240]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.22.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.22.gamma_1.weight',\n",
       "              tensor([[ 3.4235e-02, -1.3550e-02, -5.1977e-03,  ..., -2.2635e-03,\n",
       "                       -2.7881e-02,  1.4316e-02],\n",
       "                      [ 8.3157e-03,  2.0442e-02,  3.2908e-02,  ...,  7.9596e-03,\n",
       "                        2.4256e-02, -4.3507e-02],\n",
       "                      [-1.5693e-02, -2.6416e-02,  4.4011e-02,  ..., -3.6667e-02,\n",
       "                        9.0051e-03, -1.0723e-02],\n",
       "                      ...,\n",
       "                      [ 3.2935e-03,  2.9156e-02,  1.4581e-04,  ...,  3.0363e-02,\n",
       "                       -1.2676e-02, -2.0449e-02],\n",
       "                      [-3.5455e-02, -1.8179e-02, -9.0971e-03,  ..., -5.2332e-03,\n",
       "                        1.1961e-02, -6.3013e-03],\n",
       "                      [ 2.3872e-02, -4.2042e-03, -1.5708e-03,  ...,  2.1278e-02,\n",
       "                       -2.1835e-02, -4.8650e-05]], device='cuda:0')),\n",
       "             ('blocks.22.gamma_2.weight',\n",
       "              tensor([[-7.1990e-05, -2.0379e-02,  4.1953e-03,  ...,  3.6844e-02,\n",
       "                        6.9023e-03,  3.8980e-02],\n",
       "                      [-1.0656e-02,  3.7226e-02,  2.3765e-02,  ...,  2.2112e-02,\n",
       "                       -3.0679e-03,  3.5586e-02],\n",
       "                      [-1.1384e-02,  3.9869e-03, -2.3875e-02,  ..., -6.0182e-03,\n",
       "                        1.1710e-02,  9.4866e-03],\n",
       "                      ...,\n",
       "                      [-1.6523e-02, -2.3541e-02, -1.3582e-02,  ...,  3.1517e-03,\n",
       "                        1.7799e-02, -8.3119e-03],\n",
       "                      [-4.6312e-03,  1.3117e-02, -3.1151e-02,  ...,  2.5245e-02,\n",
       "                        1.9814e-02, -3.4696e-03],\n",
       "                      [ 5.9606e-03, -3.0647e-02,  1.2162e-02,  ..., -2.8658e-02,\n",
       "                       -7.8464e-03,  1.3057e-02]], device='cuda:0')),\n",
       "             ('blocks.23.norm1.weight',\n",
       "              tensor([[ 0.0389, -0.0250, -0.0233,  ..., -0.0240, -0.0126,  0.0071],\n",
       "                      [ 0.0246, -0.0249,  0.0090,  ...,  0.0075, -0.0149,  0.0062],\n",
       "                      [ 0.0287,  0.0054, -0.0052,  ..., -0.0024, -0.0285,  0.0256],\n",
       "                      ...,\n",
       "                      [-0.0145, -0.0310,  0.0109,  ..., -0.0097, -0.0060,  0.0010],\n",
       "                      [ 0.0432,  0.0034,  0.0008,  ..., -0.0073, -0.0171, -0.0112],\n",
       "                      [ 0.0025, -0.0018, -0.0127,  ..., -0.0237, -0.0192, -0.0009]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.23.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.23.attn.weight',\n",
       "              tensor([[ 0.0048,  0.0139,  0.0367,  ..., -0.0247,  0.0187,  0.0139],\n",
       "                      [ 0.0195, -0.0060, -0.0083,  ..., -0.0020,  0.0249,  0.0235],\n",
       "                      [ 0.0122,  0.0108, -0.0011,  ...,  0.0336, -0.0161,  0.0050],\n",
       "                      ...,\n",
       "                      [ 0.0328, -0.0339,  0.0178,  ..., -0.0129,  0.0235, -0.0249],\n",
       "                      [ 0.0111,  0.0186,  0.0280,  ...,  0.0044,  0.0223,  0.0021],\n",
       "                      [ 0.0316,  0.0292, -0.0053,  ..., -0.0278, -0.0076,  0.0252]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.23.attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.23.norm2.weight',\n",
       "              tensor([[ 0.0288,  0.0086, -0.0008,  ...,  0.0230, -0.0289,  0.0486],\n",
       "                      [-0.0201,  0.0111, -0.0004,  ..., -0.0149, -0.0228, -0.0002],\n",
       "                      [ 0.0203, -0.0143, -0.0195,  ..., -0.0254, -0.0277,  0.0081],\n",
       "                      ...,\n",
       "                      [-0.0014,  0.0067,  0.0209,  ...,  0.0029, -0.0088, -0.0142],\n",
       "                      [ 0.0139,  0.0331,  0.0139,  ...,  0.0072,  0.0030,  0.0182],\n",
       "                      [ 0.0184, -0.0180,  0.0085,  ...,  0.0239, -0.0014,  0.0009]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.23.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.23.mlp.fc1.weight',\n",
       "              tensor([[ 0.0194, -0.0066, -0.0058,  ...,  0.0142, -0.0034,  0.0033],\n",
       "                      [ 0.0004, -0.0441,  0.0114,  ..., -0.0020, -0.0044,  0.0533],\n",
       "                      [ 0.0033, -0.0228, -0.0181,  ..., -0.0143,  0.0031, -0.0088],\n",
       "                      ...,\n",
       "                      [ 0.0163,  0.0025, -0.0177,  ..., -0.0209,  0.0123, -0.0104],\n",
       "                      [-0.0213,  0.0052,  0.0220,  ..., -0.0187, -0.0056,  0.0237],\n",
       "                      [-0.0401, -0.0188,  0.0044,  ..., -0.0025, -0.0308, -0.0104]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.23.mlp.fc1.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')),\n",
       "             ('blocks.23.mlp.fc2.weight',\n",
       "              tensor([[-0.0021, -0.0020,  0.0257,  ..., -0.0084,  0.0109, -0.0125],\n",
       "                      [ 0.0262, -0.0032,  0.0063,  ...,  0.0306,  0.0215, -0.0058],\n",
       "                      [-0.0326, -0.0040,  0.0039,  ...,  0.0269, -0.0172, -0.0129],\n",
       "                      ...,\n",
       "                      [-0.0463, -0.0022,  0.0015,  ..., -0.0100, -0.0191, -0.0085],\n",
       "                      [-0.0120, -0.0178,  0.0368,  ..., -0.0205, -0.0242, -0.0043],\n",
       "                      [-0.0307, -0.0347,  0.0143,  ..., -0.0149, -0.0195, -0.0245]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.23.mlp.fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.23.gamma_1.weight',\n",
       "              tensor([[ 0.0222, -0.0236, -0.0240,  ..., -0.0033,  0.0325, -0.0035],\n",
       "                      [-0.0117, -0.0483, -0.0148,  ...,  0.0115, -0.0254,  0.0067],\n",
       "                      [ 0.0329, -0.0001,  0.0174,  ..., -0.0005, -0.0392, -0.0235],\n",
       "                      ...,\n",
       "                      [ 0.0051, -0.0230,  0.0066,  ..., -0.0062, -0.0114, -0.0109],\n",
       "                      [ 0.0008, -0.0277, -0.0100,  ..., -0.0015, -0.0031, -0.0246],\n",
       "                      [-0.0110, -0.0188,  0.0151,  ...,  0.0002,  0.0133,  0.0447]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.23.gamma_2.weight',\n",
       "              tensor([[-0.0428, -0.0235, -0.0289,  ..., -0.0309,  0.0380,  0.0123],\n",
       "                      [-0.0067, -0.0199, -0.0410,  ..., -0.0165,  0.0186,  0.0015],\n",
       "                      [ 0.0081, -0.0013, -0.0255,  ..., -0.0165, -0.0138, -0.0040],\n",
       "                      ...,\n",
       "                      [-0.0184, -0.0173, -0.0258,  ...,  0.0275, -0.0027,  0.0006],\n",
       "                      [-0.0087,  0.0043, -0.0074,  ...,  0.0171, -0.0200,  0.0002],\n",
       "                      [-0.0148,  0.0202,  0.0329,  ..., -0.0186,  0.0363,  0.0159]],\n",
       "                     device='cuda:0')),\n",
       "             ('norm.weight',\n",
       "              tensor([[-0.0042, -0.0318, -0.0062,  ...,  0.0283,  0.0235,  0.0119],\n",
       "                      [-0.0011, -0.0328, -0.0206,  ...,  0.0328,  0.0371, -0.0273],\n",
       "                      [-0.0179, -0.0104, -0.0359,  ..., -0.0123, -0.0010,  0.0076],\n",
       "                      ...,\n",
       "                      [-0.0053,  0.0016, -0.0299,  ...,  0.0153, -0.0193,  0.0498],\n",
       "                      [-0.0182, -0.0091, -0.0074,  ...,  0.0045,  0.0045,  0.0385],\n",
       "                      [-0.0077, -0.0086,  0.0413,  ..., -0.0031, -0.0141,  0.0147]],\n",
       "                     device='cuda:0')),\n",
       "             ('norm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('head.weight',\n",
       "              tensor([[-0.0070,  0.0107,  0.0010,  ..., -0.0168,  0.0066,  0.0210],\n",
       "                      [-0.0229, -0.0358, -0.0311,  ...,  0.0100,  0.0217, -0.0199],\n",
       "                      [ 0.0361,  0.0356, -0.0020,  ...,  0.0067, -0.0325, -0.0339],\n",
       "                      ...,\n",
       "                      [-0.0294,  0.0227,  0.0285,  ...,  0.0136,  0.0135,  0.0181],\n",
       "                      [-0.0224,  0.0097, -0.0178,  ...,  0.0049, -0.0022, -0.0037],\n",
       "                      [-0.0015, -0.0279, -0.0260,  ..., -0.0121, -0.0099,  0.0175]],\n",
       "                     device='cuda:0')),\n",
       "             ('head.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
